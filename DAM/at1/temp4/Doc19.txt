this article based exploration basic text mining capabilities r open source statistical software it intended primarily tutorial novices text mining well r however unlike conventional tutorials i spend good bit time setting context describing problem led text mining thence r i also talk limitations techniques i describe point directions exploration interested indeed ill likely explore future articles if time wish cut chase please go straight section entitled preliminaries installing r rstudio if already installed r worked may want stop reading i doubt theres anything i can tell dont already know a couple warnings order proceed r text mining options explore open source software version differences open source can significant always documented way corporate it types used indeed i tripped differences earlier version article now revised so just record examples run version r version tm text mining package r a second point follows evident version number tm package still early stages evolution as result will see things always work advertised so assume nothing inspect results detail every step be warned i always aim introduction rather accuracy background motivation traditional data analysis based relational model data stored tables within tables data stored rows row representing single record entity interest customer account the columns represent attributes entity for example customer table might consist columns name street address city postcode telephone number typically defined upfront data model created it possible add columns fact tends messy one also update existing rows information pertaining added attribute as long one asks information based existing attributes example give list customers based sydney database analyst can use structured query language defacto language relational databases get answer a problem arises however one asks information based attributes included database an example case give list customers made complaint last twelve months as result many data modelers will include catchall free text column can used capture additional information adhoc way as one might imagine column will often end containing several lines even paragraphs text near impossible analyse tools available relational databases note completeness i add database vendors incorporated text mining capabilities products indeed many now include r  another good reason learn my story over last months time permits ive indepth exploration data captured organisations it service management tool such tools capture support tickets logged track progress closed as turns number cases calls logged categories broad useful infamous catchall category called unknown in cases much important information captured free text column difficult analyse unless one knows one looking the problem i grappling identify patterns hence define subcategories enable support staff categorise calls meaningfully one way guess subcategories might   one can sometimes make pretty good guesses one knows data well enough in general however guessing terrible strategy one know one know the sensible way extract subcategories analyse content free text column systematically this classic text mining problem now i knew bit theory text mining little practical experience so logical place start look suitable text mining tool our vendor shall remain unnamed rollsroyce statistical tool good text mining addon we dont licenses tool vendor willing give us trial license months  understanding intenttopurchase basis i therefore started looking open source options while i stumbled interesting paper ingo feinerer describes text mining framework r environment now i knew r vaguely aware offered text mining capabilities id looked details anyway i started reading paper  kept going i finished as i read i realised answer problems even better require trade assorted limbs license i decided give go preliminaries installing r rstudio r can downloaded r project website there windows version available installed painlessly laptop commonly encountered installation issues answered helpful r windows faq rstudio integrated development environment ide r there commercial version product also free open source version in follows ive used free version like r rstudio installs painlessly also detects r installation rstudio following panels a script editor can create r scripts top left you can also open new script editor window going file new file rscript the console can execute r commandsscripts bottom left environment history top right files current working directory installed r packages plots help display screen bottom right check short video quick introduction rstudio you can access help anytime within r rstudio typing question mark command exercise try typing getwd setwd console i reiterate installation process products seriously simple  seriously impressive rollsroyce business intelligence vendors take lesson   addition taking long hard look ridiculous prices charge there another small step move fun stuff text mining certain plotting packages installed default one install manually the relevant packages tm text mining package see documentation also check excellent introductory article tm snowballc required stemming explained ggplot plotting capabilities see documentation wordcloud selfexplanatory see documentation warning windows users r casesensitive wordcloud wordcloud the simplest way install packages use rstudios built capabilities go tools install packages menu if youre working windows might run permissions issue installing packages if might find advice r windows faq helpful preliminaries the example dataset the data i service management tool isnt best dataset learn quite messy but i reasonable data source virtual backyard blog to end i converted posts ive written since dec plain text form posts you can download zip file i suggest create new folder called called say textmining unzip files folder that done good start  preliminaries basic navigation a things note proceed in follows i enter commands directly console however heres little rstudio tip may want consider can enter r command code fragment script editor hit ctrlenter ie hit enter key holding control key copy line console this will enable save script go along in code snippets functions commands typed r console blue font the output black i will also denote references functions commands body article italicising setwd be aware ive omitted command prompt code snippets it best cutnpaste commands directly article quotes sometimes rendered correctly a text file code article available the prompt rstudio console indicates r ready process commands to see current working directory type getwd hit return youll see something like getwd cusersdocuments the exact output will course depend working directory note forward slashes path this rs unix heritage backslash escape character r so heres change working directory cusers setwdcusers you can now use getwdto check setwd done getwd cusers i wont say much r i want get main business article check short introduction r quick crash course loading data r start rstudio open textmining project created earlier the next step load tm package loaded default this done using library function like librarytm loading required package nlp dependent packages loaded automatically case dependency nlp natural language processing package next need create collection documents technically referred corpus r environment this basically involves loading files created textmining folder corpus object the tm package provides corpus function there several ways create corpus check online help using explained earlier in nutshell corpus function can read various sources including directory thats option well use create corpus docs corpusdirsourcecuserskailashdocumentstextmining at risk stating obvious will need tailor path appropriate a couple things note any line starts comment tells r assign result command right hand side variable left hand side in case corpus object created stored variable called docs one can also use equals sign assignment one wants type docs see information newly created corpus docs vcorpus metadata corpus specific document level indexed content documents the summary function gives details including complete listing files  isnt particularly enlightening instead well examine particular document corpus inspect particular document writelinesascharacterdocs  output shown  which prints entire content th document corpus console preprocessing data cleansing though tedious perhaps important step text analysis as will see dirty data can play havoc results furthermore will also see data cleaning invariably iterative process always problems overlooked first time around the tm package offers number transformations ease tedium cleaning data to see available transformations type gettransformations r prompt gettransformations removenumbers removepunctuation removewords stemdocument stripwhitespace most selfexplanatory ill explain arent go along there preliminary cleanup steps need use powerful transformations if inspect documents corpus know now will notice i quirks writing for example i often use colons hyphens without spaces words separated using removepunctuation transform without fixing will cause two words either side symbols combined clearly need fix prior using transformations to fix one create custom transformation the tm package provides ability via contenttransformer function this function takes function input input function specify transformation needs done in case input function one replaces instances character spaces as turns gsub function just here r code build content transformer will call tospace create tospace content transformer tospace contenttransformerfunctionx pattern return gsubpattern x now can use content transformer eliminate colons hypens like docs tmmapdocs tospace docs tmmapdocs tospace inspect random sections f corpus check result intend use writelines shown earlier to reiterate something i mentioned preamble good practice inspect subset corpus transformation if looks good can now apply removepunctuation transformation this done follows remove punctuation replace punctuation marks docs tmmapdocs removepunctuation inspecting corpus reveals several nonstandard punctuation marks removed these include single curly quote marks spacehyphen combination these can removed using custom content transformer tospace note might want copynpaste symbols directly relevant text file ensure accurately represented tospace docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace inspect corpus ensure offenders eliminated this also good time check special symbols may need removed manually if well can move next step convert corpus lower case remove numbers since r case sensitive text equal text hence rationale converting standard case however although tolower transformation part standard tm transformations see output gettransformations previous section for reason convert tolower transformation can handle corpus object properly this done help new friend contenttransformer heres relevant code transform lower case need wrap contenttransformer docs tmmapdocscontenttransformertolower text analysts typically interested numbers since usually contribute meaning text however may always for example definitely case one interested getting count number times particular year appears corpus this need wrapped contenttransformer standard transformation tm strip digits std transformation need contenttransformer docs tmmapdocs removenumbers once sure inspect corpus proceeding the next step remove common words text these include words articles conjunctions etc common verbs qualifiers yet however etc the tm package includes standard list stop words referred we remove stop words using standard removewords transformation like remove stopwords using standard list tm docs tmmapdocs removewords stopwordsenglish finally remove extraneous whitespaces using stripwhitespace transformation strip whitespace cosmetic docs tmmapdocs stripwhitespace stemming typically large corpus will contain many words common root example offer offered offering stemming process reducing related words common root case word offer simple stemming algorithms one tm relatively crude work chopping ends words this can cause problems example words mate mating might reduced mat instead mate that said overall benefit gained stemming makes downside special cases to see stemming lets take look last lines corpus stemming heres last bit looks like prior stemming note may differ depending ordering corpus source files directory writelinesascharacterdocs flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun now lets stem corpus reinspect load library librarysnowballc stem document docs tmmapdocsstemdocument writelinesascharacterdocs flexibl eye behold action increas organis flexibl say redeploy employe like seen affect move constrain individu flexibl dual mean characterist mani organiz platitud excel synergi andgovern interest exercis analys platitud expos differ espous actual mean sign wish mani hour platitud deconstruct fun a careful comparison two paragraphs reveals benefits tradeoff relatively crude process there sophisticated procedure called lemmatization takes grammatical context account among things determining lemma word requires knowledge part speech pos ie whether noun adjective etc there pos taggers automate process tagging terms parts speech although pos taggers available r see one example i will go topic make long post even longer on another important note output corpus also shows problem two first organiz organis actually variants stem organ clearly merged second word andgovern separated govern error original text these errors ilk can fixed proceeding this easily done using gsub wrapped contenttransformer here code clean issues i found docs tmmapdocs contenttransformergsub pattern organiz replacement organ docs tmmapdocs contenttransformergsub pattern organis replacement organ docs tmmapdocs contenttransformergsub pattern andgovern replacement govern docs tmmapdocs contenttransformergsub pattern inenterpris replacement enterpris docs tmmapdocs contenttransformergsub pattern team replacement team note i removed stop words rd th transforms there definitely errors need cleaned ill leave detect remove the document term matrix the next step process creation document term matrix dtm matrix lists occurrences words corpus document in dtm documents represented rows terms words columns if word occurs particular document matrix entry corresponding row column else multiple occurrences within document recorded word occurs twice document recorded relevant matrix entry a simple example might serve explain structure tdm clearly assume simple corpus consisting two documents doc doc following content doc bananas yellow doc bananas good the dtm corpus look like bananas yellow good doc doc clearly nothing special rows columns just easily transpose if wed get term document matrix tdm terms rows documents columns one can work either dtm tdm ill use dtm follows there couple general points worth making proceed firstly dtms tdms can huge dimension matrix number document x number words corpus secondly clear large majority words will appear documents as result dtm invariably sparse large number entries the business creating dtm tdm r simple dtm documenttermmatrixdocs this creates term document matrix corpus stores result variable dtm one can get summary information matrix typing variable name console hitting return dtm documenttermmatrix documents terms nonsparse entries sparsity maximal term length weighting term frequency tf this x dimension matrix rows zero one can inspect dtm might want fun however isnt particularly illuminating sheer volume information will flash console to limit information displayed one can inspect small section like inspectdtm documenttermmatrix documents terms nonsparse entries sparsity maximal term length weighting term frequency tf docs creation creativ credibl credit crimin crinkl beyondentitiesandrelationshipstxt bigdatatxt this command displays terms first two rows dtm note results may differ mining corpus notice constructing tdm converted corpus text mathematical object can analysed using quantitative techniques matrix algebra it surprise therefore tdm dtm starting point quantitative text analysis for example get frequency occurrence word corpus simply sum rows give column sums freq colsumsasmatrixdtm here first converted tdm mathematical matrix using asmatrix function we summed rows give us totals column term the result stored column matrix variable freq check dimension freq equals number terms length total number terms lengthfreq next sort freq descending order term count create sort order descending ord orderfreqdecreasingtrue then list least frequently occurring terms inspect frequently occurring terms freqheadord one organ can manag work system inspect least frequently occurring terms freqtailord yield yorkshir youtub zeno zero zulli the least frequent terms can interesting one might think this terms occur rarely likely descriptive specific documents indeed i can recall posts i referred yorkshire zenos paradox mr lou zulli without go back corpus id hard time enumerating posts ive used word system there least couple ways simple ways strike balance frequency specificity one way use socalled inverse document frequencies a simpler approach eliminate words occur large fraction corpus documents the latter addresses another issue evident we deal now words like can one give us information subject matter documents occur they can therefore eliminated without loss indeed eliminated stopword removal earlier however since words occur frequently virtually documents can remove enforcing bounds creating dtm like dtmr documenttermmatrixdocs controllistwordlengthsc bounds listglobal c here told r include words occur documents we also enforced lower upper limit length words included characters inspecting new dtm dtmr documenttermmatrix documents terms nonsparse entries sparsity maximal term length weighting term frequency tf the dimension reduced x lets calculate cumulative frequencies words across documents sort freqr colsumsasmatrixdtmr length total number terms lengthfreqr create sort order asc ordr orderfreqrdecreasingtrue inspect frequently occurring terms freqrheadordr organ manag work system project problem inspect least frequently occurring terms freqrtailordr wait warehous welcom whiteboard wider widespread the results make sense top keywords pretty good descriptors blogs projects management systems however high frequency words need significant what give idea potential classification terms that done lets take get list terms occur least times entire corpus this easily done using findfreqterms function follows findfreqtermsdtmrlowfreq action approach base busi chang consult data decis design develop differ discuss enterpris exampl group howev import issu like make manag mani model often organ peopl point practic problem process project question said system thing think time understand view well will work here i asked findfreqterms return terms occur times entire corpus note however result ordered alphabetically frequency now frequently occurring terms hand can check correlations terms occur corpus in context correlation quantitative measure cooccurrence words multiple documents the tm package provides findassocs function one needs specify dtm term interest correlation limit the latter number serves lower bound strength correlation search result terms for example correlation limit findassocs will return words always cooccur search term a correlation limit will return terms search term cooccurrence least here results running findassocs frequently occurring terms system project organis correlation findassocsdtmrproject project inher handl manag occurr manager findassocsdtmrenterpris enterpris agil realist increment upfront technolog neither solv adapt architectur happi movement architect chanc fine featur findassocsdtmrsystem system design subset adopt user involv specifi function intend softwar step compos intent specif depart phone frequent today pattern cognit wherea an important point note presence term list indicative frequency rather measure frequency two search result term cooccur show together documents across note also indicator nearness contiguity indeed document term matrix store information proximity terms simply bag words that said one can already see correlations throw interesting combinations example project manag enterpris agil architectarchitecture system design adopt these give one insights potential classifications as turned basic techniques listed enough get handle original problem led text mining analysis free text problem descriptions organisations service management tool what i work way top terms find associations these revealed number sets keywords occurred multiple problem descriptions good enough define useful subcategories these currently reviewed service management team while theyre busy im looking refining using techniques cluster analysis tokenization a simple case latter look twoword combinations text technically referred bigrams as one might imagine dimensionality dtm will quickly get hand one considers larger multiword combinations anyway will topics wait future articles piece much long already that said one thing i absolutely must touch upon signing do stay i think youll find interesting basic graphics one really cool things r graphing capability ill just couple simple examples give flavour power cool factor there lots nice examples web can try lets first simple frequency histogram ill use ggplot package written hadley wickham heres code wfdataframetermnamesfreqroccurrencesfreqr libraryggplot p ggplotsubsetwf freqr aesterm occurrences p p geombarstatidentity p p themeaxistextxelementtextangle hjust p figure shows result fig termoccurrence histogram freq fig termoccurrence histogram freq the first line creates data frame list columns equal length a data frame also contains name columns case term occurrence respectively we invoke ggplot telling consider plot terms occur times the aes option ggplot describes plot aesthetics case use specify x y axis labels the statidentity option geombar ensures height bar proportional data value mapped yaxis ie occurrences the last line specifies xaxis labels degree angle horizontally justified see happens leave check voluminous ggplot documentation better yet quick introduction ggplot edwin chen finally lets create wordcloud reason everyone can seems the code wordcloud librarywordcloud setting seed time ensures consistent look across clouds setseed limit words specifying min frequency wordcloudnamesfreqrfreqr minfreq the result shown figure fig wordcloud freq fig wordcloud freq here first load wordcloud package loaded default setting seed number ensures get look time try running without setting seed the arguments wordcloud function straightforward enough note one can specify maximum number words included instead minimum frequency i done see word cloud documentation this word cloud also makes clear stop word removal done job well number words missed also however example these can removed augmenting builtin stop word list custom one this left exercise reader finally one can make wordcloud visually appealing adding colour follows  add color wordcloudnamesfreqrfreqrminfreqcolorsbrewerpaldark<U+2033> the result shown figure fig wordcloud freq fig wordcloud freq you may need load rcolorbrewer package get work check brewer documentation experiment colouring options wrapping this brings end rather long i hope comprehensible introduction text mining r it clear despite length article ive covered rudimentary basics nevertheless i hope ive succeeded conveying sense possibilities vast rapidlyexpanding discipline text analytics
