a couple months ago i wrote article highlighting pitfalls using risk matrices risk matrices example scoring methods techniques use ordinal scales assess risks in methods risks ranked predefined criteria impact expected loss ranking used basis decisions risks addressed scoring methods popular easy use however douglas hubbard points critique current risk management practices many commonly used scoring techniques flawed  post based hubbards critique research papers quoted therein brief look flaws risk scoring techniques commonly used risk scoring techniques problems associated scoring techniques fall two major categories weighted scores these use several ordered scales weighted according perceived importance for example  might asked rate financial risk technical risk organisational risk scale weight factors respectively possibly cfo happens project sponsor concerned financial risk risks  point scores weights assigned  highly subjective risk matrices these rank risks along two dimensions probability impact assign qualitative ranking high medium low depending fall coxs theorem shows categorisations internally inconsistent category boundaries arbitrarily chosen hubbard makes point although methods endorsed many standards methodologies including used project management used caution flawed to quote book together ordinalscoring methods benchmark analysis risks andor decisions least component large organizations thousands people certified methods based part computing risk scores like  major management consulting firms influenced virtually standards since standards common used various scoring schemes instead actual quantitative risk analysis methods i  call collectively scoring methods and without exception borderline worthless in practices may make many decisions far worse using merely unaided judgements what basis claim hubbard points following scoring methods make allowance flawed perceptions analysts assign scores ie consider effect cognitive bias i wont dwell i previously written effect cognitive biases project risk management see post  example qualitative descriptions assigned score understood differently different people further rarely objective guidance analyst distinguish high medium risk such advice may even help research budescu broomell po shows  huge variances understanding qualitative descriptions even people given specific guidelines descriptions terms mean scoring methods add errors below brief descriptions in paper risk matrix theorem cox mentions typical risk matrices  correctly unambiguously compare small fraction eg less randomly selected pairs hazards they  assign identical ratings quantitatively different risks he calls behaviour range compression applies scoring technique uses ranges assigned scores tend cluster around midlow high range analysis hubbard shows point scale responses  implies changing score viceversa  disproportionate effect classification risks scores implicitly assume magnitude quantity assumed directly proportional scale for example score implies criterion measured twice large score however reality criteria rarely linear implied scale scoring techniques often presume factors scored independent ie correlations factors  assumption rarely tested justified way many project management standards advocate use scoring techniques to fair many situations adequate long used understanding limitations seen light hubbards book admonition standards textbook writers critical methods advocate warning practitioners uncritical adherence standards best practices best way manage project risks scoring done right just clear hubbards criticism directed scoring methods use arbitrary qualitative scales justified independent analysis there techniques though superficially similar flawed scoring methods actually quite robust based observations use real measures opposed arbitrary ones alignment business objectives scale without defining alignment means validated fact hence refined use as example sound scoring technique hubbard quotes paper dawes presents evidence linear scoring models superior intuition clinical judgements strangely although weights  obtained intuition scoring model outperforms clinical intuition  happens human intuition good identifying important factors hot evaluating net effect several possibly competing factors hence simple linear scoring models  outperform intuition  key models validated checking predictions reality another class techniques use axioms based logic reduce inconsistencies decisions an example technique multiattribute utility theory since based logic methods  also considered solid foundation unlike discussed previous section conclusions many commonly used scoring methods risk analysis based flaky theoretical foundations worse none to compound problem often used without validation a particularly ubiquitous example wellknown loved risk matrix in paper risk matrices tony cox shows risk matrices  sometimes lead decisions worse made basis coin toss  fact possibility even small  worry anyone uses risk matrices flawed scoring techniques without understanding limitations
