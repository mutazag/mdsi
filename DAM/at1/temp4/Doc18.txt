 term machine learning gets lot airtime popular trade press days as i started writing article i quick search recent news headlines contained term here top three results datelines within three days search httpventurebeatcombeyondthegimmickimplementingeffectivemachinelearningvblive httpwwwinfoworldcomarticleartificialintelligencenewbigdatatoolsformachinelearningspringfromhomeofsparkandmesoshtml httpwwwinfoworldcomarticleanalyticsreviewthebestframeworksformachinelearninganddeeplearninghtml  truth hype usually tends quite prosaic case machine learning professor yaser abumostafa puts simply learning data and although professor referring computers humans learn patterns discerned sensory data as states first lines wonderful mathematically demanding book entitled learning from data if show picture threeyearold ask theres tree  likely get correct answer if ask thirty year old definition tree  likely get inconclusive answer we didnt learn tree studying model trees we learned looking trees in words learned data in words three year old forms model constitutes tree process discerning common pattern objects grownups around label trees data she  predict something tree applying model new instances presented  exactly happens machine learning computer correctly algorithm builds predictive model variable like treeness based patterns discerns data  model  applied predict value variable eg tree new instances with said introduction worth contrasting machinedriven process model building traditional approach building mathematical models predict phenomena say physics engineering what models good physicists engineers model phenomena using physical laws mathematics  aim modelling understand predict natural phenomena for example physical law newtons law gravitation model helps us understand gravity works make predictions say mars going six months now indeed theories laws physics models wide applicability aside models typically expressed via differential equations most differential equations hard solve analytically exactly scientists use computers solve numerically it important note case computers used calculation tools play role modelbuilding as mentioned earlier role models sciences twofold understanding prediction in contrast machine learning focus usually prediction rather understanding  predictive successes machine learning led certain commentators claim scientific theory building obsolete science  advance crunching data alone such claims overblown mention hubristic although data scientist may able predict accuracy may able tell particular prediction obtained  lack understanding  mislead  even harmful consequences point thats worth unpacking detail assumptions assumptions a model real world process phenomenon necessarily simplification  essentially impossible isolate process phenomenon rest world as consequence impossible know certain model  built incorporated interactions influence process phenomenon interest it quite possible potentially important variables overlooked  selection variables go model based assumptions in case model building physics assumptions made upfront thus clear anybody takes trouble read underlying theory in machine learning however assumptions harder see implicit data algorithm   problem data biased algorithm opaque problem bias opacity become acute datasets increase size algorithms become complex especially applied social issues serious human consequences i wont go examples interested reader may want look cathy oneils book weapons math destruction article dark side data science as aside i point although assumptions usually obvious traditional modelling often overlooked sheer laziness charitably lack awareness   disastrous consequences  global financial crisis  extent blamed failure trading professionals understand assumptions behind model used calculate value collateralised debt obligations it starts straight line now weve taken tour key differences model building old new worlds set start talking machine learning proper i begin admitting i overstated point opacity machine learning algorithms transparent  possibly indeed chances know algorithm im going discuss next either introductory statistics course university plotting relationships two variables favourite spreadsheet yea may guessed im referring linear regression in simplest avatar linear regression attempts fit straight line set data points two dimensions  two dimensions correspond dependent variable traditionally denoted y independent variable traditionally denoted x an example fitted line shown figure once line obtained   predict value dependent variable value independent variable in terms earlier discussion line model figure linear regression figure linear regression figure also serves illustrate linear models going inappropriate real world situations straight line fit data well but hard devise methods fit complicated functions  important point since machine learning finding functions accurately predict dependent variables yet unknown values independent variables algorithms make explicit implicit choices form functions complexity versus simplicity at first sight seems nobrainer complicated functions  work better simple ones after choose nonlinear function lots parameters able fit complex data set better linear function  see figure complicated function fits datapoints better straight line but theres catch although ability fit dataset increases flexibility fitting function increasing complexity beyond point  invariably reduce predictive power put another way complex enough function may fit known data points perfectly consequence  inevitably perform poorly unknown data  important point lets look greater detail figure simple complex fitting functions figure simple complex fitting function courtesy wikimedia recall aim machine learning predict values dependent variable yet unknown values independent variables given finite usually limited dataset build model  confidence  usual strategy partition dataset two subsets  containing data called training set containing remainder called test set  model built ie appropriate function fitted using training data verified test data  verification process consists comparing predicted values dependent variable known values test set now intuitively clear complicated function better  fit training data question why answer because complicated functions free parameters example linear functions single dependent variable two parameters slope intercept quadratics three cubics four  mathematician john von neumann believed said with four parameters i  fit elephant five i  make wiggle trunk see post nice demonstration literal truth words put another way complex functions wrigglier simple ones suitable adjustment parameters wriggliness  adjusted fit training data better functions less wriggly figure illustrates point well  may sound like  cake eat choose complicated enough function  fit training test data well not keep mind resulting model fitted function built using training set alone good fit test data guaranteed in fact intuitively clear function fits training data perfectly figure likely terrible job test data question why answer remember far model concerned test data unknown hence greater wriggliness trained model less likely fit test data well remember model fitted training data freedom tweak parameters  tension simplicity complexity models  key principles machine learning called biasvariance tradeoff bias refers lack flexibility variance reducible error in general simpler functions greater bias lower variance complex functions opposite much subtlety machine learning lies developing understanding arrive right level complexity problem hand tweak parameters resulting function fits training data just well enough generalise well unknown data note curious learn biasvariance tradeoff may want look piece for details achieve optimal tradeoff search articles regularization machine learning unlocking unstructured data  discussion thus far focused primarily quantitative enumerable data numbers categories thats stored structured format ie columns rows spreadsheet database table  fine goes fact much data organisations unstructured common examples text documents audiovisual media  data virtually impossible analyse computationally using relational database technologies sql commonly used organisations  situation changed dramatically last decade text analysis techniques required expensive software highend computers now implemented open source languages python r  run personal computers for problems require computing power memory beyond cloud technologies make possible cheaply in opinion ability analyse textual data important advance data technologies last decade it unlocks world possibilities curious data analyst just think comment fields survey data  now analysed way never possible relational world there general impression text analysis hard although advanced techniques  take little time wrap ones head around basics simple enough yea i really mean proof check tutorial topic wrapping i go indeed i planning delve algorithms increasing complexity regression trees forests neural nets close brief peek recent headlinegrabbing developments like deep learning however i realised exploration long perhaps importantly defeat main intent piece give starting students idea machine learning differs preexisting techniques data analysis i hope i succeeded least partially achieving aim for interested learning machine learning algorithms i  suggest look gentle introduction data science using r series articles start  text analysis link last line previous section move clustering topic modelling naive bayes decision trees random forests support vector machines im slowly adding list i find time please check back time time 
