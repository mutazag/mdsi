the term machine learning gets lot airtime popular trade press days as i started writing article i quick search recent news headlines contained term here top three results datelines within three days search httpventurebeatcombeyondthegimmickimplementingeffectivemachinelearningvblive httpwwwinfoworldcomarticleartificialintelligencenewbigdatatoolsformachinelearningspringfromhomeofsparkandmesoshtml httpwwwinfoworldcomarticleanalyticsreviewthebestframeworksformachinelearninganddeeplearninghtml the truth hype usually tends quite prosaic case machine learning professor yaser abumostafa puts simply learning data and although professor referring computers humans learn patterns discerned sensory data as states first lines wonderful mathematically demanding book entitled learning from data if show picture threeyearold ask theres tree will likely get correct answer if ask thirty year old definition tree will likely get inconclusive answer we didnt learn tree studying model trees we learned looking trees in words learned data in words three year old forms model constitutes tree process discerning common pattern objects grownups around label trees data she can predict something tree applying model new instances presented this exactly happens machine learning computer correctly algorithm builds predictive model variable like treeness based patterns discerns data the model can applied predict value variable eg tree new instances with said introduction worth contrasting machinedriven process model building traditional approach building mathematical models predict phenomena say physics engineering what models good physicists engineers model phenomena using physical laws mathematics the aim modelling understand predict natural phenomena for example physical law newtons law gravitation model helps us understand gravity works make predictions say mars going six months now indeed theories laws physics models wide applicability aside models typically expressed via differential equations most differential equations hard solve analytically exactly scientists use computers solve numerically it important note case computers used calculation tools play role modelbuilding as mentioned earlier role models sciences twofold understanding prediction in contrast machine learning focus usually prediction rather understanding the predictive successes machine learning led certain commentators claim scientific theory building obsolete science can advance crunching data alone such claims overblown mention hubristic although data scientist may able predict accuracy may able tell particular prediction obtained this lack understanding can mislead can even harmful consequences point thats worth unpacking detail  assumptions assumptions a model real world process phenomenon necessarily simplification this essentially impossible isolate process phenomenon rest world as consequence impossible know certain model one built incorporated interactions influence process phenomenon interest it quite possible potentially important variables overlooked the selection variables go model based assumptions in case model building physics assumptions made upfront thus clear anybody takes trouble read underlying theory in machine learning however assumptions harder see implicit data algorithm this can problem data biased algorithm opaque problem bias opacity become acute datasets increase size algorithms become complex especially applied social issues serious human consequences i wont go examples interested reader may want look cathy oneils book weapons math destruction article dark side data science as aside i point although assumptions usually obvious traditional modelling often overlooked sheer laziness charitably lack awareness this can disastrous consequences the global financial crisis can extent blamed failure trading professionals understand assumptions behind model used calculate value collateralised debt obligations it starts straight line  now weve taken tour key differences model building old new worlds set start talking machine learning proper i begin admitting i overstated point opacity machine learning algorithms transparent can possibly indeed chances know algorithm im going discuss next either introductory statistics course university plotting relationships two variables favourite spreadsheet yea may guessed im referring linear regression in simplest avatar linear regression attempts fit straight line set data points two dimensions the two dimensions correspond dependent variable traditionally denoted y independent variable traditionally denoted x an example fitted line shown figure once line obtained one can predict value dependent variable value independent variable in terms earlier discussion line model figure linear regression figure linear regression figure also serves illustrate linear models going inappropriate real world situations straight line fit data well but hard devise methods fit complicated functions the important point since machine learning finding functions accurately predict dependent variables yet unknown values independent variables algorithms make explicit implicit choices form functions complexity versus simplicity at first sight seems nobrainer complicated functions will work better simple ones after choose nonlinear function lots parameters able fit complex data set better linear function can see figure complicated function fits datapoints better straight line but theres catch although ability fit dataset increases flexibility fitting function increasing complexity beyond point will invariably reduce predictive power put another way complex enough function may fit known data points perfectly consequence will inevitably perform poorly unknown data this important point lets look greater detail figure simple complex fitting functions figure simple complex fitting function courtesy wikimedia recall aim machine learning predict values dependent variable yet unknown values independent variables given finite usually limited dataset build model can confidence the usual strategy partition dataset two subsets one containing data called training set containing remainder called test set the model built ie appropriate function fitted using training data verified test data the verification process consists comparing predicted values dependent variable known values test set now intuitively clear complicated function better will fit training data question why answer because complicated functions free parameters example linear functions single dependent variable two parameters slope intercept quadratics three cubics four the mathematician john von neumann believed said with four parameters i can fit elephant five i can make wiggle trunk see post nice demonstration literal truth words put another way complex functions wrigglier simple ones suitable adjustment parameters wriggliness can adjusted fit training data better functions less wriggly figure illustrates point well this may sound like can cake eat choose complicated enough function can fit training test data well not keep mind resulting model fitted function built using training set alone good fit test data guaranteed in fact intuitively clear function fits training data perfectly figure likely terrible job test data question why answer remember far model concerned test data unknown hence greater wriggliness trained model less likely fit test data well remember model fitted training data freedom tweak parameters this tension simplicity complexity models one key principles machine learning called biasvariance tradeoff bias refers lack flexibility variance reducible error in general simpler functions greater bias lower variance complex functions opposite much subtlety machine learning lies developing understanding arrive right level complexity problem hand tweak parameters resulting function fits training data just well enough generalise well unknown data note curious learn biasvariance tradeoff may want look piece for details achieve optimal tradeoff search articles regularization machine learning unlocking unstructured data the discussion thus far focused primarily quantitative enumerable data numbers categories thats stored structured format ie columns rows spreadsheet database table this fine goes fact much data organisations unstructured common examples text documents audiovisual media this data virtually impossible analyse computationally using relational database technologies sql commonly used organisations the situation changed dramatically last decade text analysis techniques required expensive software highend computers now implemented open source languages python r can run personal computers for problems require computing power memory beyond cloud technologies make possible cheaply in opinion ability analyse textual data important advance data technologies last decade it unlocks world possibilities curious data analyst just think comment fields survey data can now analysed way never possible relational world there general impression text analysis hard although advanced techniques can take little time wrap ones head around basics simple enough yea i really mean proof check tutorial topic wrapping i go indeed i planning delve algorithms increasing complexity regression trees forests neural nets close brief peek recent headlinegrabbing developments like deep learning however i realised exploration long perhaps importantly defeat main intent piece give starting students idea machine learning differs preexisting techniques data analysis i hope i succeeded least partially achieving aim for interested learning machine learning algorithms i can suggest look gentle introduction data science using r series articles start one text analysis link last line previous section move clustering topic modelling naive bayes decision trees random forests support vector machines im slowly adding list i find time please check back time time 
