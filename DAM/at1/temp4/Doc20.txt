welcome second part introductory series text analysis using r first article can accessed my aim present piece provide practical introduction cluster analysis ill begin background moving nuts bolts clustering we fair bit cover lets get right a common problem analysing large collections documents categorize meaningful way this easy enough one predefined classification scheme known fit collection collection small enough browsed manually one can simply scan documents looking keywords appropriate category classify documents based results more often however classification scheme available collection large one needs use algorithms can classify documents automatically based structure content the present post practical introduction couple automatic text categorization techniques often referred clustering algorithms as wikipedia article clustering tells us cluster analysis clustering task grouping set objects way objects group called cluster similar sense another groups clusters as one might guess results clustering depend rather critically method one uses group objects again quoting wikipedia piece cluster analysis one specific algorithm general task solved it can achieved various algorithms differ significantly notion constitutes cluster efficiently find popular notions clusters include groups small distances note well use distancebased methods among cluster members dense areas data space intervals particular statistical distributions ie distributions words within documents entire collection   bit later   notion cluster precisely defined one reasons many clustering algorithms there common denominator group data objects however different researchers employ different cluster models cluster models different algorithms can given the notion cluster found different algorithms varies significantly properties understanding cluster models key understanding differences various algorithms an upshot always straightforward interpret output clustering algorithms indeed will see example discussed with said introduction lets move nut bolts clustering preprocessing corpus in section i cover steps required create r objects necessary order clustering it goes territory ive covered detail first article series albeit tweaks may want skim even youve read previous piece to begin ill assume r rstudio free development environment r installed computer familiar basic functionality text mining ™ package if need help please look instructions previous article text mining as first part series i will use posts blog example collection corpus text miningspeak the corpus can downloaded for completeness i will run entire sequence steps right loading corpus r running two clustering algorithms ready lets go  the first step fire rstudio navigate directory unpacked example corpus once done load text mining package tm heres relevant code note complete listing code article can accessed getwd cuserskailashdocuments set working directory fix path needed setwdcuserskailashdocumentstextmining load tm library librarytm loading required package nlp note r commands blue output black red lines start comments if get error probably need download install tm package you can rstudio going tools install packages entering tm when installing new package r automatically checks installs dependent packages the next step load collection documents object can manipulated functions tm package create corpus docs corpusdirsourcecuserskailashdocumentstextmining inspect particular document writelinesascharacterdocs   the next step clean corpus this includes things transforming consistent case removing nonstandard symbols punctuation removing numbers assuming numbers contain useful information case transform lower case docs tmmapdocscontenttransformertolower remove potentiallyy problematic symbols tospace contenttransformerfunctionx pattern return gsubpattern x docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace • docs tmmapdocs tospace • docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace remove punctuation docs tmmapdocs removepunctuation strip digits docs tmmapdocs removenumbers note please see previous article contenttransformer tospace function defined next remove stopwords common words like example eliminate extraneous whitespaces remove stopwords docs tmmapdocs removewords stopwordsenglish remove whitespace docs tmmapdocs stripwhitespace writelinesascharacterdocs flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun at point critical inspect corpus stopword removal tm can flaky yes annoying showstopper one can remove problematic words manually one identified minute next stem document ie truncate words base form for example education educate educative stemmed educat docs tmmapdocsstemdocument stemming works well enough fixes need done due inconsistent use britishaussie us english also well take opportunity fix concatenations like andgovernance see paragraph printed heres code docs tmmapdocs contenttransformergsubpattern organiz replacement organ docs tmmapdocs contenttransformergsub pattern organis replacement organ docs tmmapdocs contenttransformergsub pattern andgovern replacement govern docs tmmapdocs contenttransformergsub pattern inenterpris replacement enterpris docs tmmapdocs contenttransformergsub pattern team replacement team the next step remove stopwords missed r the best way small corpus go compile list words eliminated one can create custom vector containing words removed use removewords transformation needful here code note indicates continuation statement previous line mystopwords ccan sayonewayuse alsohowevtellwill muchneedtaketendeven likeparticularrathersaid getwellmakeaskcomeend firsttwohelpoftenmay mightseesomeththingpoint postlookrightnowthinkve re remove custom stopwords docs tmmapdocs removewords mystopwords again good idea check offending words really eliminated the final preprocessing step create documentterm matrix dtm matrix lists occurrences words corpus in dtm documents represented rows terms words columns if word occurs particular document n times matrix entry corresponding row column n doesnt occur entry creating dtm straightforward one simply uses builtin documenttermmatrix function provided tm package like dtm documenttermmatrixdocs print summary dtm nonsparse entries sparsity maximal term length weighting term frequency tf this brings us end preprocessing phase next ill briefly explain distancebased algorithms work going actual work clustering an intuitive introduction algorithms as mentioned introduction basic idea behind document text clustering categorise documents groups based likeness lets take brief look algorithms work magic consider structure dtm very briefly matrix documents represented rows words columns in case corpus documents words dtm x matrix mathematically one can think matrix describing dimensional space words represents coordinate axis document represented point space this hard visualise course may help illustrate via twodocument corpus three words total consider following corpus document a five plus five document b five plus six these two documents can represented points dimensional space words five plus six three coordinate axes see figure figure documents a b points word space figure documents a b points word space now documents can thought point space easy enough take next logical step define notion distance two points ie two documents in figure distance a b i denote dabis length line connecting two points simply sum squares differences coordinates two points representing documents dab sqrt sqrt generalising dimensional space hand distance two documents lets call x y coordinates word frequencies xxx yyy one can define straight line distance also called euclidean distance dxy dxy sqrtx yx yx y it noted euclidean distance i described possible way define distance mathematically there many others take far afield discuss see article dont put term metric metric context merely distance whats important idea one can define numerical distance documents once grasped easy understand basic idea behind clustering algorithms work group documents based distancerelated criteria to sure explanation simplistic glosses complicated details algorithms nevertheless reasonable approximate explanation goes hood i hope purists reading will agree finally completeness i mention many clustering algorithms distancebased hierarchical clustering the first algorithm well look hierarchical clustering as wikipedia article topic tells us strategies hierarchical clustering fall two types agglomerative start document cluster the algorithm iteratively merges documents clusters closest entire corpus forms single cluster each merge happens different increasing distance divisive start entire set documents single cluster at step algorithm splits cluster recursively document cluster this basically inverse agglomerative strategy the algorithm well use hclust agglomerative hierarchical clustering heres simplified description works assign document single member cluster find pair clusters closest merge so now one cluster less compute distances new cluster old clusters repeat steps single cluster containing documents well need things running algorithm firstly need convert dtm standard matrix can used dist distance computation function r dtm stored standard matrix well also shorten document names display nicely graph will use display results hclust names i given documents just way long heres relevant code convert dtm matrix m asmatrixdtm write csv file optional writecsvmfiledtmeightlatecsv shorten rownames display purposes rownamesm pastesubstringrownamesmrepnrowm substringrownamesm ncharrownamesmncharrownamesm compute distance document vectors d distm next run hclust the algorithm offers several options check documentation details i use popular option called wards method others i suggest experiment gives slightly different results making interpretation somewhat tricky i mention clustering much art science finally visualise results dendogram see figure run hierarchical clustering using wards method groups hclustdmethodwardd plot dendogram use hang ensure labels fall tree plotgroups hang figure dendogram hierarchical clustering corpus figure dendogram hierarchical clustering corpus a words interpreting dendrograms hierarchical clusters work way tree figure branch point encounter distance cluster merge occurred clearly welldefined clusters largest separation many closely spaced branch points indicate lack dissimilarity ie distance case clusters based figure reveals welldefined clusters first one consisting three documents right end cluster second containing documents we can display clusters graph using recthclust function like cut subtrees try recthclustgroups the result shown figure figure cluster solution figure cluster grouping the figures show grouping clusters figure cluster solution figure cluster grouping figure cluster solution figure cluster grouping ill make just one point cluster grouping seems robust one happens large distance cleanly separated distancewise cluster grouping that said ill leave explore ins outs hclust move next algorithm k means clustering in hierarchical clustering specify number clusters upfront these determined looking dendogram algorithm done work in contrast next algorithm k means requires us define number clusters upfront number k name the algorithm generates k document clusters way ensures withincluster distances cluster member centroid geometric mean cluster minimised heres simplified description algorithm assign documents randomly k bins compute location centroid bin compute distance document centroid assign document bin corresponding centroid closest stop document moved new bin else go step an important limitation k means method solution found algorithm corresponds local rather global minimum figure wikipedia explains difference two nice succinct way as consequence important run algorithm number times time different starting configuration select result gives overall lowest sum withincluster distances documents a simple check solution robust run algorithm increasing number initial configurations result change significantly that said procedure guarantee globally optimal solution i reckon thats enough said algorithm lets get using the relevant function might well guessed kmeans as always i urge check documentation understand available options well use default options parameters excepting nstart set we also plot result using clusplot function cluster library may need install reminder can install packages via toolsinstall packages menu rstudio k means algorithm clusters starting configurations kfit kmeansd nstart plot need library cluster librarycluster clusplotasmatrixd kfitcluster colort shadet labels lines the plot shown figure figure principal component plot k figure principal component plot k the cluster plot shown figure needs bit explanation as mentioned earlier clustering algorithms work mathematical space whose dimensionality equals number words corpus case clearly impossible visualize to handle mathematicians invented dimensionality reduction technique called principal component analysis reduces number dimensions case way reduced dimensions capture much variability clusters possible hence comment two components explain point variability bottom plot figure aside yes i realize figures hard read overly long names i leave fix no excuses know   running algorithm plotting results k yields figures figure principal component plot k figure principal component plot k figure principal component plot k figure principal component plot k choosing k recall k means algorithm requires us specify k upfront a natural question best choice k in truth onesizefitsall answer question heuristics might sometimes help guide choice for completeness ill describe one even though much help clustering problem in simplified description k means algorithm i mentioned technique attempts minimise sum distances points cluster clusters centroid actually quantity minimised total withincluster sum squares wss point mean intuitively one might expect quantity maximum k decrease k increases sharply first less sharply k reaches optimal value the problem reasoning often happens within cluster sum squares never shows slowing decrease summed wss unfortunately exactly happens case hand i reckon picture might help make clearer below r code draw plot summed wss function k k way total number documents kmeans determine optimum number clusters elbow method look elbow plot summed intracluster distances withinss fn k wss wssi sumkmeansdcentersinstartwithinss plot wss typeb xlabnumber clustersylabwithin groups sum squares   figure shows resulting plot figure wss function k elbow plot the plot clearly shows k summed wss flattens distinct elbow as result method help fortunately case one can get sensible answer using common sense rather computation choice clusters seems optimal algorithms yield exactly clusters show clearest cluster separation point review dendogram cluster plots k the meaning now i must acknowledge elephant room i steadfastly ignored thus far the odds good youve seen already  it topics themes two clusters correspond unfortunately question straightforward answer although algorithms suggest cluster grouping silent topics themes related moreover will see experiment results clustering depend the criteria construction dtm see documentation documenttermmatrix options the clustering algorithm indeed insofar clustering concerned subject matter corpus knowledge best way figure cluster themes this serves reinforce yet clustering much art science in case hand article length seems important differentiator clusters found algorithms the three articles smaller cluster top longest pieces corpus additionally three pieces related sensemaking dialogue mapping there probably factors well none stand significant i mention however fact article length seems play significant role suggests may worth checking effect scaling distances word counts using measures cosine similarity thats topic another post note added dec check article visualizing relationships documents using network graphs detailed discussion cosine similarity the take home lesson results clustering often hard interpret this surprising algorithms interpret meaning simply chug mathematical optimisation problem the onus analyst figure means  means anything conclusion this brings us end long ramble clustering weve explored two common methods hierarchical k means clustering many others available r i urge explore apart providing detailed steps clustering i attempted provide intuitive explanation algorithms work i hope i succeeded as always feedback welcome finally id like reiterate important point results clustering exercise straightforward interpretation often case cluster analysis fortunately i can close optimistic note there text mining techniques better job grouping documents based topics themes rather word frequencies alone ill discuss next article series until i wish many enjoyable hours exploring ins outs clustering
