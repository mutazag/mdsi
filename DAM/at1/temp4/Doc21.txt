 standard way search documents internet via keywords keyphrases  pretty much google search engines routinely well however useful limitations consider example situation confronted large collection documents idea  first things might want classify documents topics themes among things help figure theres anything interest also directing relevant subsets corpus for small collections  simply going document clearly infeasible corpuses containing thousands documents topic modeling theme post deals problem automatically classifying sets documents themes  article organised follows i first provide background topic modelling  algorithm i use latent dirichlet allocation lda involves pretty heavy maths ill avoid altogether however i  provide intuitive explanation lda works moving practical example uses topicmodels library r as previous articles series see post  i  discuss steps detail along explanations provide accessible references concepts covered space blog post aside beware lda also abbreviation linear discriminant analysis classification technique i hope cover later ongoing series text data analytics latent dirichlet allocation mathfree introduction in essence lda technique facilitates automatic discovery themes collection documents  basic assumption behind lda documents collection consist mixture collectionwide topics however reality observe documents words topics latter part hidden latent structure documents  aim infer latent topic structure given words document lda recreating documents corpus adjusting relative importance topics documents words topics iteratively heres brief explanation algorithm works quoted directly answer edwin chen quora go document randomly assign word document  k topics note  shortcomings lda  specify number topics denoted k upfront more later  assignment already gives topic representations documents word distributions topics albeit good ones so improve document d go word w d and topic t compute two things ptopic t document d proportion words document d currently assigned topic t pword w topic t proportion assignments topic t documents come word w reassign w new topic choose topic t probability ptopic t document d pword w topic t according generative model essentially probability topic t generated word w makes sense resample current words topic probability note pab conditional probability given b already occurred see post conditional probabilities in words step assuming topic assignments except current word question correct updating assignment current word using model documents generated after repeating previous step large number times youll eventually reach roughly steady state assignments pretty good so use assignments estimate topic mixtures document counting proportion words assigned topic within document words associated topic counting proportion words assigned topic overall for another simple explanation lda works check article matthew jockers for technical exposition take look video david blei  inventors algorithm  iterative process described last point implemented using technique called gibbs sampling ill say bit gibbs sampling later may want look paper philip resnick eric hardesty explains nittygritty algorithm warning involves fair bit math good intuitive explanations well as general point i also emphasise need understand ins outs algorithm use help understand least high level algorithm  needs develop feel algorithms even  doesnt understand details indeed people working analytics know details algorithms use doesnt stop using algorithms intelligently purists may disagree i think wrong finally youre doubt wondering term dirichlet lda refers fact topics words assumed follow dirichlet distributions there good reason apart convenience dirichlet distributions provide good approximations word distributions documents perhaps important computationally convenient preprocessing as previous articles text mining i  use collection posts blog example corpus  corpus  downloaded i  assume r rstudio installed follow link need help  preprocessing steps much described previous articles nevertheless ill risk boring detailed listing  reproduce results load text mining library librarytm set working directory modify path needed setwdcuserskailashdocumentstextmining load files corpus get listing txt files directory filenames listfilesgetwdpatterntxt read files character vector files lapplyfilenamesreadlines create corpus vector docs corpusvectorsourcefiles inspect particular document corpus writelinesascharacterdocs start preprocessing transform lower case docs tmmapdocscontenttransformertolower remove potentially problematic symbols tospace contenttransformerfunctionx pattern return gsubpattern x docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace docs tmmapdocs tospace • docs tmmapdocs tospace docs tmmapdocs tospace remove punctuation docs tmmapdocs removepunctuation strip digits docs tmmapdocs removenumbers remove stopwords docs tmmapdocs removewords stopwordsenglish remove whitespace docs tmmapdocs stripwhitespace good practice check every now writelinesascharacterdocs stem document docs tmmapdocsstemdocument fix differences us aussie english general errors docs tmmapdocs contenttransformergsub pattern organiz replacement organ docs tmmapdocs contenttransformergsub pattern organis replacement organ docs tmmapdocs contenttransformergsub pattern andgovern replacement govern docs tmmapdocs contenttransformergsub pattern inenterpris replacement enterpris docs tmmapdocs contenttransformergsub pattern team replacement team define eliminate custom stopwords mystopwords ccan sayonewayuse alsohowevtellwill muchneedtaketendeven likeparticularrathersaid getwellmakeaskcomeend firsttwohelpoftenmay mightseesomeththingpoint postlookrightnowthink ve re anothputsetnewgood wantsurekindlargyesdayetc quitsincattemptlackseenawar littlevermoreovthoughfoundabl enoughfarearliawayachievdraw lastneverbriefbitentirbrief greatlot docs tmmapdocs removewords mystopwords inspect document check writelinesascharacterdocs create documentterm matrix dtm documenttermmatrixdocs convert rownames filenames rownamesdtm filenames collapse matrix summing columns freq colsumsasmatrixdtm length total number terms lengthfreq create sort order descending ord orderfreqdecreasingtrue list terms decreasing order freq write disk freqord writecsvfreqordwordfreqcsv check preprocessing section either article  detailed explanations code  document term matrix dtm produced code  main input lda algorithm next section topic modelling using lda we now ready topic modelling well use topicmodels package written bettina gruen kurt hornik specifically well use lda function gibbs sampling option mentioned earlier ill say second  lda function fairly large number parameters ill describe briefly for please check vignette gruen hornik for part well use default parameter values supplied lda functioncustom setting parameters required gibbs sampling algorithm gibbs sampling works performing random walk way reflects characteristics desired distribution because starting point walk chosen random necessary discard first steps walk correctly reflect properties distribution  referred burnin period we set burnin parameter following burnin period perform iterations taking every th iteration use  reason avoid correlations samples we use different starting points nstart five independent runs each starting point requires seed integer also ensures reproducibility i provided random integers seed list finally ive set best true actually default setting instructs algorithm return results run highest posterior probability some words caution order it emphasised settings guarantee convergence algorithm globally optimal solution indeed gibbs sampling  best find locally optimal solution even hard prove mathematically specific practical problems  dealing  upshot best lots runs different settings parameters check stability results  bottom line interest purely practical good enough results make sense well leave issues mathematical rigour better qualified deal as mentioned earlier important parameter must specified upfront k number topics algorithm use classify documents there mathematical approaches often yield semantically meaningful choices k see post stackoverflow example from practical point view   simply run algorithm different values k make choice based inspecting results  well ok first step set parameters r lets also load topicmodels library note might need install package part base r installation load topic models library librarytopicmodels set parameters gibbs sampling burnin iter thin seed list nstart best true number topics k that done  now actual work run topic modelling algorithm corpus here code run lda using gibbs sampling ldaout ldadtmk methodgibbs controllistnstartnstart seed seed bestbest burnin burnin iter iter thinthin write results docs topics ldaouttopics asmatrixtopicsldaout writecsvldaouttopicsfilepasteldagibbskdocstotopicscsv top terms topic ldaoutterms asmatrixtermsldaout writecsvldaouttermsfilepasteldagibbsktopicstotermscsv probabilities associated topic assignment topicprobabilities asdataframeldaoutgamma writecsvtopicprobabilitiesfilepasteldagibbsktopicprobabilitiescsv find relative importance top topics topictotopic lapplynrowdtmfunctionx sorttopicprobabilitiesxksorttopicprobabilitiesxk find relative importance second third important topics topictotopic lapplynrowdtmfunctionx sorttopicprobabilitiesxksorttopicprobabilitiesxk write file writecsvtopictotopicfilepasteldagibbsktopictotopiccsv writecsvtopictotopicfilepasteldagibbsktopictotopiccsv  lda algorithm returns object contains lot information of particular interest us document topic assignments top terms topic probabilities associated terms these printed first three calls writecsv there important points note each document considered mixture topics case  assignments first file list top topic  highest probability point each topic contains terms words corpus albeit different probabilities we list top terms second file  last file lists probabilities topic assigned document  therefore x matrix docs topics as  might expect highest probability row corresponds topic assigned document  goodness primary assignment discussed point  assessed taking ratio highest secondhighest probability secondhighest thirdhighest probability  ive done last nine lines code take time examine output confirm primary topic assignments best ratios probabilities discussed point highest you also experiment different values k see  find better topic distributions in interests space i  restrict k  table lists top terms topics topic topic topic topic topic work question chang system project practic map organ data manag mani time consult model approach flexibl ibi manag design organ differ issu work process decis best plan problem busi problem  table lists document primary topic assignments document topic beyondentitiesandrelationshipstxt bigdatatxt conditionsovercausestxt emergentdesigninenterpriseittxt frominformationtoknowledgetxt fromthecoalfacetxt heraclitusandparmenidestxt ironiesofenterpriseittxt makingsenseoforganizationalchangetxt makingsenseofsensemakingtxt objectivityandtheethicaldimensionofdecisionmakingtxt ontheinherentambiguitiesofmanagingprojectstxt organisationalsurprisetxt professionalsorpoliticianstxt ritualsininformationsystemdesigntxt routinesandrealitytxt scapegoatsandsystemstxt sherlockholmesfailedprojectstxt sherlockholmesmgmtfetistxt sixheresiesforbitxt sixheresiesforenterprisearchitecturetxt thearchitectandtheapparitiontxt thecloudandthegrasstxt theconsultantsdilemmatxt thedangerwithintxt thedilemmasofenterpriseittxt theessenceofentrepreneurshiptxt threetypesofuncertaintytxt togafornottogaftxt understandingflexibilitytxt from quick perusal two tables appears algorithm done pretty decent job for exampletopic data system design documents assigned topic however far perfect example interview i neil preston organisational change makingsenseoforganizationalchangetxt assigned topic seems project management it associated topic change lets see  resolve looking probabilities associated topics  table lists topic probabilities document topic topic topic topic topic beyonden bigdata conditio emergent frominfo fromthec heraclit ironieso makingse makingse objectiv ontheinh organisa professi ritualsi routines scapegoa sherlock sherlock sixheres sixheres thearchi thecloud theconsu thedange thedilem theessen threetyp togaforn understa in table highest probability row bold also cases maximum secondthird largest probabilities close i highlighted second third highest probabilities red it clear neils interview th document table topics comparable probabilities topic project management topic change topic issue mapping ibis decreasing order probabilities in general document multiple topics comparable probabilities simply means document speaks topics proportions indicated probabilities a reading neils interview  convince conversation indeed range topics that said algorithm far perfect you might already noticed poor assignments here  post sherlock holmes case failed project assigned topic i reckon belongs topic there number others i wont belabor point except reiterate precisely definitely want experiment different settings iteration parameters check stability important try range different values k find optimal number topics to conclude topic modelling provides quick convenient way perform unsupervised classification corpus documents as always though  needs examine results carefully check make sense id like end general observation classifying documents ageold concern cuts across disciplines so surprise topic modelling got lookin diverse communities indeed i reading learning lda i found best introductory articles area written academics working english departments   things i love working text analysis wealth material web written diverse perspectives  term crossdisciplinary often tends platitude case simply statement fact i hope i able convince explore rapidly evolving field exciting times ahead come join fun
