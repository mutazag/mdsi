A couple months ago I wrote article highlighting pitfalls using risk matrices. Risk matrices example scoring methods , techniques use ordinal scales assess risks. In methods, risks ranked predefined criteria impact expected loss, ranking used basis decisions risks addressed. Scoring methods popular easy use. However, Douglas Hubbard points critique current risk management practices, many commonly used scoring techniques flawed. This post   based Hubbard's critique research papers quoted therein   brief look flaws risk scoring techniques. Commonly used risk scoring techniques problems associated Scoring techniques fall two major categories: Weighted scores: These use several ordered scales weighted according perceived importance. For example: one might asked rate financial risk, technical risk organisational risk scale 1 5 , weight factors 0.6, 0.3 0.1 respectively (possibly CFO   happens project sponsor   concerned financial risk risks ). The point , scores weights assigned can highly subjective   . Risk matrices: These rank risks along two dimensions   probability impact   assign qualitative ranking high, medium low depending fall. Cox's theorem shows categorisations internally inconsistent category boundaries arbitrarily chosen. Hubbard makes point , although methods endorsed many standards methodologies (including used project management), used caution flawed. To quote book: Together ordinal/scoring methods benchmark analysis risks / decisions least component large organizations. Thousands people certified methods based part computing risk scores like . The major management consulting firms influenced virtually standards. Since standards common used various scoring schemes instead actual quantitative risk analysis methods, I will call collectively "scoring methods." And , without exception, borderline worthless. In practices, may make many decisions far worse using merely unaided judgements. What basis claim? Hubbard points following: Scoring methods make allowance flawed perceptions analysts assign scores   .e. consider effect cognitive bias. I dwell I previously written effect cognitive biases project risk management -see post one, example. Qualitative descriptions assigned score understood differently different people. Further, rarely objective guidance analyst distinguish high medium risk. Such advice may even help: research Budescu, Broomell Po shows can huge variances understanding qualitative descriptions, even people given specific guidelines descriptions terms mean. Scoring methods add errors. Below brief descriptions : In paper risk matrix theorem, Cox mentions "Typical risk matrices can correctly unambiguously compare small fraction (e.g., less 10%) randomly selected pairs hazards. They can assign identical ratings quantitatively different risks." He calls behaviour "range compression"   applies scoring technique uses ranges. Assigned scores tend cluster around mid-low high range. Analysis Hubbard shows , 5 point scale, 75% responses 3 4. This implies changing score 3 4 vice-versa can disproportionate effect classification risks. Scores implicitly assume magnitude quantity assumed directly proportional scale. For example, score 2 implies criterion measured twice large score 1. However, reality, criteria rarely linear implied scale. Scoring techniques often presume factors scored independent   .e. correlations factors. This assumption rarely tested justified way. Many project management standards advocate use scoring techniques. To fair, many situations adequate long used understanding limitations. Seen light, Hubbard's book admonition standards textbook writers critical methods advocate, warning practitioners uncritical adherence standards best practices best way manage project risks . Scoring done right Just clear, Hubbard's criticism directed scoring methods use arbitrary, qualitative scales justified independent analysis. There techniques , though superficially similar flawed scoring methods, actually quite robust : Based observations. Use real measures ( opposed arbitrary ones   "alignment business objectives" scale 1 5, without defining "alignment" means.) Validated fact ( hence refined use). As example sound scoring technique, Hubbard quotes paper Dawes, presents evidence linear scoring models superior intuition clinical judgements. Strangely, although weights can obtained intuition, scoring model outperforms clinical intuition. This happens human intuition good identifying important factors, hot evaluating net effect several, possibly competing factors. Hence simple linear scoring models can outperform intuition. The key models validated checking predictions reality. Another class techniques use axioms based logic reduce inconsistencies decisions. An example technique multi-attribute utility theory. Since based logic, methods can also considered solid foundation unlike discussed previous section. Conclusions Many commonly used scoring methods risk analysis based flaky theoretical foundations   worse, none . To compound problem, often used without validation. A particularly ubiquitous example well-known loved risk matrix. In paper risk matrices, Tony Cox shows risk matrices can sometimes lead decisions worse made basis coin toss. The fact possibility   even small one   worry anyone uses risk matrices ( flawed scoring techniques) without understanding limitations.
