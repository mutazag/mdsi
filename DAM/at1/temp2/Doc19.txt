This article based exploration basic text mining capabilities R, open source statistical software. It intended primarily tutorial novices text mining well R. However, unlike conventional tutorials, I spend good bit time setting context describing problem led text mining thence R. I also talk limitations techniques I describe, point directions exploration interested. Indeed, I'll likely explore future articles. If time / wish cut chase, please go straight section entitled, Preliminaries   installing R RStudio. If already installed R worked , may want stop reading I doubt anything I can tell already know A couple warnings order proceed. R text mining options explore open source software. Version differences open source can significant always documented way corporate IT types used . Indeed, I tripped differences earlier version article (now revised). So, just record, examples run version 3.2.0 R version 0.6-1 tm (text mining) package R. A second point follows : evident version number, tm package still early stages evolution. As result   will see   things always work advertised. So assume nothing, inspect results detail every step. Be warned I always , aim introduction rather accuracy. Background motivation Traditional data analysis based relational model data stored tables. Within tables, data stored rows   row representing single record entity interest ( customer account). The columns represent attributes entity. For example, customer table might consist columns name, street address, city, postcode, telephone number . Typically defined upfront, data model created. It possible add columns fact, tends messy one also update existing rows information pertaining added attribute. As long one asks information based existing attributes   example , "give list customers based Sydney"   database analyst can use Structured Query Language ( defacto language relational databases ) get answer. A problem arises, however, one asks information based attributes included database. An example case : "give list customers made complaint last twelve months." As result , many data modelers will include "catch-" free text column can used capture additional information ad-hoc way. As one might imagine, column will often end containing several lines, even paragraphs text near impossible analyse tools available relational databases. (Note: completeness I add database vendors incorporated text mining capabilities products. Indeed, many now include R  another good reason learn .) My story Over last months, time permits, I've -depth exploration data captured organisation's IT service management tool. Such tools capture support tickets logged, track progress closed. As turns , number cases calls logged categories broad useful   infamous catch- category called "Unknown." In cases, much important information captured free text column, difficult analyse unless one knows one looking . The problem I grappling identify patterns hence define sub-categories enable support staff categorise calls meaningfully. One way guess sub-categories might   one can sometimes make pretty good guesses one knows data well enough. In general, however, guessing terrible strategy one know one know. The sensible way extract subcategories analyse content free text column systematically. This classic text mining problem. Now, I knew bit theory text mining, little practical experience . So logical place start look suitable text mining tool. Our vendor ( shall remain unnamed) "Rolls-Royce" statistical tool good text mining add-. We licenses tool, vendor willing give us trial license months  understanding intent--purchase basis. I therefore started looking open source options. While , I stumbled interesting paper Ingo Feinerer describes text mining framework R environment. Now, I knew R, vaguely aware offered text mining capabilities, I'd looked details. Anyway, I started reading paper  kept going I finished. As I read, I realised answer problems. Even better, require trade assorted limbs license. I decided give go. Preliminaries   installing R RStudio R can downloaded R Project website. There Windows version available, installed painlessly laptop. Commonly encountered installation issues answered ( helpful) R Windows FAQ. RStudio integrated development environment (IDE) R. There commercial version product, also free open source version. In follows, I've used free version. Like R, RStudio installs painlessly also detects R installation. RStudio following panels: A script editor can create R scripts (top left). You can also open new script editor window going File > New File > RScript. The console can execute R commands/scripts (bottom left) Environment history (top right) Files current working directory, installed R packages, plots help display screen (bottom right). Check short video quick introduction RStudio. You can access help anytime (within R RStudio) typing question mark command. Exercise: try typing ?getwd() ?setwd() console. I reiterate installation process products seriously simple  seriously impressive. "Rolls-Royce" business intelligence vendors take lesson   addition taking long hard look ridiculous prices charge. There another small step move fun stuff. Text mining certain plotting packages installed default one install manually The relevant packages : tm   text mining package (see documentation). Also check excellent introductory article tm. SnowballC   required stemming (explained ). ggplot2   plotting capabilities (see documentation) wordcloud   self-explanatory (see documentation) . (Warning Windows users: R case-sensitive Wordcloud != wordcloud) The simplest way install packages use RStudio's built capabilities (go Tools > Install Packages menu). If working Windows 7 8, might run permissions issue installing packages. If , might find advice R Windows FAQ helpful. Preliminaries   The example dataset The data I service management tool best dataset learn quite messy. But , I reasonable data source virtual backyard: blog. To end, I converted posts I've written since Dec 2013 plain text form (30 posts ). You can download zip file . I suggest create new folder called   called, say, TextMining   unzip files folder. That done, good start  Preliminaries   Basic Navigation A things note proceed: In follows, I enter commands directly console. However, little RStudio tip may want consider: can enter R command code fragment script editor hit Ctrl-Enter (.e. hit Enter key holding Control key) copy line console. This will enable save script go along. In code snippets , functions / commands typed R console blue font. The output black. I will also denote references functions / commands body article italicising "setwd()". Be aware I've omitted command prompt ">" code snippets ! It best cut-n-paste commands directly article quotes sometimes rendered correctly. A text file code article available . The > prompt RStudio console indicates R ready process commands. To see current working directory type getwd() hit return. You'll see something like: getwd() [1] "C:/Users/Documents" The exact output will course depend working directory. Note forward slashes path. This R's Unix heritage (backslash escape character R.). So, change working directory C:\Users: setwd("C:/Users") You can now use getwd() check setwd() done . getwd() [1]"C:/Users" I say much R I want get main business article. Check short introduction R quick crash course. Loading data R Start RStudio open TextMining project created earlier. The next step load tm package loaded default. This done using library() function like : library(tm) Loading required package: NLP Dependent packages loaded automatically   case dependency NLP (natural language processing) package. Next, need create collection documents (technically referred Corpus) R environment. This basically involves loading files created TextMining folder Corpus object. The tm package provides Corpus() function . There several ways create Corpus (check online help using ? explained earlier). In nutshell, Corpus() function can read various sources including directory. That's option use: #Create Corpus docs <- Corpus(DirSource("C:/Users/Kailash/Documents/TextMining")) At risk stating obvious, will need tailor path appropriate. A couple things note . Any line starts # comment, "<-" tells R assign result command right hand side variable left hand side. In case Corpus object created stored variable called docs. One can also use equals sign (=) assignment one wants . Type docs see information newly created corpus: docs <<VCorpus>> Metadata: corpus specific: 0, document level (indexed): 0 Content: documents: 30 The summary() function gives details, including complete listing files  particularly enlightening. Instead, examine particular document corpus. #inspect particular document writeLines(.character(docs[[30]]))  output shown  Which prints entire content 30th document corpus console. Pre-processing Data cleansing, though tedious, perhaps important step text analysis. As will see, dirty data can play havoc results. Furthermore, will also see, data cleaning invariably iterative process always problems overlooked first time around. The tm package offers number transformations ease tedium cleaning data. To see available transformations type getTransformations() R prompt: > getTransformations() [1] "removeNumbers" "removePunctuation" "removeWords" "stemDocument" "stripWhitespace" Most self-explanatory. I'll explain go along. There preliminary clean- steps need use powerful transformations. If inspect documents corpus ( know now), will notice I quirks writing. For example, I often use colons hyphens without spaces words separated . Using removePunctuation transform without fixing will cause two words either side symbols combined. Clearly, need fix prior using transformations. To fix , one create custom transformation. The tm package provides ability via content_transformer function. This function takes function input, input function specify transformation needs done. In case, input function one replaces instances character spaces. As turns gsub() function just . Here R code build content transformer, will call toSpace: #create toSpace content transformer toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) Now can use content transformer eliminate colons hypens like : docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, ":") Inspect random sections f corpus check result intend (use writeLines shown earlier). To reiterate something I mentioned preamble, good practice inspect subset corpus transformation. If looks good, can now apply removePunctuation transformation. This done follows: #Remove punctuation   replace punctuation marks " " docs <- tm_map(docs, removePunctuation) Inspecting corpus reveals several "non-standard" punctuation marks removed. These include single curly quote marks space-hyphen combination. These can removed using custom content transformer, toSpace. Note might want copy-n-paste symbols directly relevant text file ensure accurately represented toSpace. docs <- tm_map(docs, toSpace, "'") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, " -") Inspect corpus ensure offenders eliminated. This also good time check special symbols may need removed manually. If well, can move next step : Convert corpus lower case Remove numbers. Since R case sensitive, "Text" equal "text"   hence rationale converting standard case. However, although tolower transformation, part standard tm transformations (see output getTransformations() previous section). For reason, convert tolower transformation can handle corpus object properly. This done help new friend, content_transformer. Here's relevant code: #Transform lower case (need wrap content_transformer) docs <- tm_map(docs,content_transformer(tolower)) Text analysts typically interested numbers since usually contribute meaning text. However, may always . For example, definitely case one interested getting count number times particular year appears corpus. This need wrapped content_transformer standard transformation tm. #Strip digits (std transformation, need content_transformer) docs <- tm_map(docs, removeNumbers) Once , sure inspect corpus proceeding. The next step remove common words text. These include words articles (, , ), conjunctions (, etc.), common verbs (), qualifiers (yet, however etc) . The tm package includes standard list stop words referred . We remove stop words using standard removeWords transformation like : #remove stopwords using standard list tm docs <- tm_map(docs, removeWords, stopwords("english")) Finally, remove extraneous whitespaces using stripWhitespace transformation: #Strip whitespace (cosmetic?) docs <- tm_map(docs, stripWhitespace) Stemming Typically large corpus will contain many words common root   example: offer, offered offering. Stemming process reducing related words common root, case word offer. Simple stemming algorithms ( one tm) relatively crude: work chopping ends words. This can cause problems: example, words mate mating might reduced mat instead mate. That said, overall benefit gained stemming makes downside special cases. To see stemming , take look last lines corpus stemming. Here's last bit looks like prior stemming (note may differ , depending ordering corpus source files directory): writeLines(.character(docs[[30]])) flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun Now stem corpus reinspect . #load library library(SnowballC) #Stem document docs <- tm_map(docs,stemDocument) writeLines(.character(docs[[30]])) flexibl eye behold action increas organis flexibl say redeploy employe like seen affect move constrain individu flexibl dual mean characterist mani organiz platitud excel synergi andgovern interest exercis analys platitud expos differ espous actual mean sign wish mani hour platitud deconstruct fun A careful comparison two paragraphs reveals benefits tradeoff relatively crude process. There sophisticated procedure called lemmatization takes grammatical context account. Among things, determining lemma word requires knowledge part speech (POS)   .e. whether noun, adjective etc. There POS taggers automate process tagging terms parts speech. Although POS taggers available R (see one, example), I will go topic make long post even longer. On another important note, output corpus also shows problem two. First, organiz organis actually variants stem organ. Clearly, merged. Second, word andgovern separated govern ( error original text). These ( errors ilk) can fixed proceeding. This easily done using gsub() wrapped content_transformer. Here code clean issues I found: docs <- tm_map(docs, content_transformer(gsub), pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") Note I removed stop words 3rd 4th transforms . There definitely errors need cleaned , I'll leave detect remove. The document term matrix The next step process creation document term matrix (DTM)  matrix lists occurrences words corpus, document. In DTM, documents represented rows terms ( words) columns. If word occurs particular document, matrix entry corresponding row column 1, else 0 (multiple occurrences within document recorded   , word occurs twice document, recorded "2" relevant matrix entry). A simple example might serve explain structure TDM clearly. Assume simple corpus consisting two documents, Doc1 Doc2, following content: Doc1: bananas yellow Doc2: bananas good The DTM corpus look like: bananas yellow good Doc1 1 1 1 0 Doc2 1 1 0 1 Clearly nothing special rows columns   just easily transpose . If , get term document matrix (TDM) terms rows documents columns. One can work either DTM TDM. I'll use DTM follows. There couple general points worth making proceed. Firstly, DTMs ( TDMs) can huge   dimension matrix number document x number words corpus. Secondly, clear large majority words will appear documents. As result DTM invariably sparse   , large number entries 0. The business creating DTM ( TDM) R simple : dtm <- DocumentTermMatrix(docs) This creates term document matrix corpus stores result variable dtm. One can get summary information matrix typing variable name console hitting return: dtm <<DocumentTermMatrix (documents: 30, terms: 4209)>> Non-/sparse entries: 14252/112018 Sparsity : 89% Maximal term length: 48 Weighting : term frequency (tf) This 30 x 4209 dimension matrix 89% rows zero. One can inspect DTM, might want fun. However, particularly illuminating sheer volume information will flash console. To limit information displayed, one can inspect small section like : inspect(dtm[1:2,1000:1005]) <<DocumentTermMatrix (documents: 2, terms: 6)>> Non-/sparse entries: 0/12 Sparsity : 100% Maximal term length: 8 Weighting : term frequency (tf) Docs creation creativ credibl credit crimin crinkl BeyondEntitiesAndRelationships.txt 0 0 0 0 0 0 bigdata.txt 0 0 0 0 0 0 This command displays terms 1000 1005 first two rows DTM. Note results may differ. Mining corpus Notice constructing TDM, converted corpus text mathematical object can analysed using quantitative techniques matrix algebra. It surprise, therefore, TDM ( DTM) starting point quantitative text analysis. For example, get frequency occurrence word corpus, simply sum rows give column sums: freq <- colSums(.matrix(dtm)) Here first converted TDM mathematical matrix using .matrix() function. We summed rows give us totals column (term). The result stored (column matrix) variable freq. Check dimension freq equals number terms: #length total number terms length(freq) [1] 4209 Next, sort freq descending order term count: #create sort order (descending) ord <- order(freq,decreasing=TRUE) Then list least frequently occurring terms: #inspect frequently occurring terms freq[head(ord)] one organ can manag work system 314 268 244 222 202 193 #inspect least frequently occurring terms freq[tail(ord)] yield yorkshir youtub zeno zero zulli 1 1 1 1 1 1 The least frequent terms can interesting one might think. This terms occur rarely likely descriptive specific documents. Indeed, I can recall posts I referred Yorkshire, Zeno's Paradox Mr. Lou Zulli without go back corpus, I'd hard time enumerating posts I've used word system. There least couple ways simple ways strike balance frequency specificity. One way use -called inverse document frequencies. A simpler approach eliminate words occur large fraction corpus documents. The latter addresses another issue evident . We deal now. Words like "can" "one" give us information subject matter documents occur. They can therefore eliminated without loss. Indeed, eliminated stopword removal earlier. However, since words occur frequently   virtually documents   can remove enforcing bounds creating DTM, like : dtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(4, 20), bounds = list(global = c(3,27)))) Here told R include words occur 3 27 documents. We also enforced lower upper limit length words included ( 4 20 characters). Inspecting new DTM: dtmr <<DocumentTermMatrix (documents: 30, terms: 1290)>> Non-/sparse entries: 10002/28698 Sparsity : 74% Maximal term length: 15 Weighting : term frequency (tf) The dimension reduced 30 x 1290. Let's calculate cumulative frequencies words across documents sort : freqr <- colSums(.matrix(dtmr)) #length total number terms length(freqr) [1] 1290 #create sort order (asc) ordr <- order(freqr,decreasing=TRUE) #inspect frequently occurring terms freqr[head(ordr)] organ manag work system project problem 268 222 202 193 184 171 #inspect least frequently occurring terms freqr[tail(ordr)] wait warehous welcom whiteboard wider widespread 3 3 3 3 3 3 The results make sense: top 6 keywords pretty good descriptors blogs   projects, management systems. However, high frequency words need significant. What , give idea potential classification terms. That done, take get list terms occur least 100 times entire corpus. This easily done using findFreqTerms() function follows: findFreqTerms(dtmr,lowfreq=80) [1] "action" "approach" "base" "busi" "chang" "consult" "data" "decis" "design" [10] "develop" "differ" "discuss" "enterpris" "exampl" "group" "howev" "import" "issu" [19] "like" "make" "manag" "mani" "model" "often" "organ" "peopl" "point" [28] "practic" "problem" "process" "project" "question" "said" "system" "thing" "think" [37] "time" "understand" "view" "well" "will" "work" Here I asked findFreqTerms() return terms occur 80 times entire corpus. Note, however, result ordered alphabetically, frequency. Now frequently occurring terms hand, can check correlations terms occur corpus. In context, correlation quantitative measure co-occurrence words multiple documents. The tm package provides findAssocs() function . One needs specify DTM, term interest correlation limit. The latter number 0 1 serves lower bound strength correlation search result terms. For example, correlation limit 1, findAssocs() will return words always co-occur search term. A correlation limit 0.5 will return terms search term co-occurrence least 50% . Here results running findAssocs() frequently occurring terms (system, project, organis) correlation 60%. findAssocs(dtmr,"project",0.6) project inher 0.82 handl 0.68 manag 0.68 occurr 0.68 manager' 0.66 findAssocs(dtmr,"enterpris",0.6) enterpris agil 0.80 realist 0.78 increment 0.77 upfront 0.77 technolog 0.70 neither 0.69 solv 0.69 adapt 0.67 architectur 0.67 happi 0.67 movement 0.67 architect 0.66 chanc 0.65 fine 0.64 featur 0.63 findAssocs(dtmr,"system",0.6) system design 0.78 subset 0.78 adopt 0.77 user 0.77 involv 0.71 specifi 0.71 function 0.70 intend 0.67 softwar 0.67 step 0.67 compos 0.66 intent 0.66 specif 0.66 depart 0.65 phone 0.63 frequent 0.62 today 0.62 pattern 0.61 cognit 0.60 wherea 0.60 An important point note presence term list indicative frequency. Rather measure frequency two (search result term) co-occur ( show together) documents across . Note also, indicator nearness contiguity. Indeed, document term matrix store information proximity terms, simply "bag words." That said, one can already see correlations throw interesting combinations   example, project manag, enterpris agil architect/architecture, system design adopt. These give one insights potential classifications. As turned , basic techniques listed enough get handle original problem led text mining   analysis free text problem descriptions organisation's service management tool. What I work way top 50 terms find associations. These revealed number sets keywords occurred multiple problem descriptions, good enough define useful sub-categories. These currently reviewed service management team. While busy , I'm looking refining using techniques cluster analysis tokenization. A simple case latter look two-word combinations text (technically referred bigrams). As one might imagine, dimensionality DTM will quickly get hand one considers larger multi-word combinations. Anyway, will topics wait future articles piece much long already. That said, one thing I absolutely must touch upon signing . Do stay, I think find interesting. Basic graphics One really cool things R graphing capability. I'll just couple simple examples give flavour power cool factor. There lots nice examples Web can try . Let's first simple frequency histogram. I'll use ggplot2 package, written Hadley Wickham . Here's code: wf=data.frame(term=names(freqr),occurrences=freqr) library(ggplot2) p <- ggplot(subset(wf, freqr>100), aes(term, occurrences)) p <- p + geom_bar(stat="identity") p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) p Figure 1 shows result. Fig 1: Term-occurrence histogram (freq>100) Fig 1: Term-occurrence histogram (freq>100) The first line creates data frame   list columns equal length. A data frame also contains name columns   case term occurrence respectively. We invoke ggplot(), telling consider plot terms occur 100 times. The aes option ggplot describes plot aesthetics   case, use specify x y axis labels. The stat="identity" option geom_bar () ensures height bar proportional data value mapped y-axis (.e occurrences). The last line specifies x-axis labels 45 degree angle horizontally justified (see happens leave ). Check voluminous ggplot documentation better yet, quick introduction ggplot2 Edwin Chen. Finally, create wordcloud reason everyone can seems . The code : #wordcloud library(wordcloud) #setting seed time ensures consistent look across clouds set.seed(42) #limit words specifying min frequency wordcloud(names(freqr),freqr, min.freq=70) The result shown Figure 2. Fig 2: Wordcloud (freq>70) Fig 2: Wordcloud (freq>70) Here first load wordcloud package loaded default. Setting seed number ensures get look time (try running without setting seed). The arguments wordcloud() function straightforward enough. Note one can specify maximum number words included instead minimum frequency ( I done ). See word cloud documentation . This word cloud also makes clear stop word removal done job well, number words missed (also however, example). These can removed augmenting built- stop word list custom one. This left exercise reader :-). Finally, one can make wordcloud visually appealing adding colour follows: # add color wordcloud(names(freqr),freqr,min.freq=70,colors=brewer.pal(6,"Dark2<U+2033>)) The result shown Figure 3. Fig 3: Wordcloud (freq > 70) Fig 3: Wordcloud (freq > 70) You may need load RColorBrewer package get work. Check brewer documentation experiment colouring options. Wrapping This brings end rather long ( I hope, comprehensible) introduction text mining R. It clear despite length article, I've covered rudimentary basics. Nevertheless, I hope I've succeeded conveying sense possibilities vast rapidly-expanding discipline text analytics.
