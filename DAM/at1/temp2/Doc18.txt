The term machine learning gets lot airtime popular trade press days. As I started writing article, I quick search recent news headlines contained term. Here top three results datelines within three days search: http://venturebeat.com/2017/02/01/beyond--gimmick-implementing-effective-machine-learning-vb-live/ http://www.infoworld.com/article/3164249/artificial-intelligence/new-big-data-tools--machine-learning-spring--home--spark--mesos.html http://www.infoworld.com/article/3163525/analytics/review--best-frameworks--machine-learning--deep-learning.html The truth hype usually tends quite prosaic case. Machine learning, Professor Yaser Abu-Mostafa puts , simply "learning data." And although professor referring computers, humans   learn patterns discerned sensory data. As states first lines wonderful ( mathematically demanding!) book entitled, Learning From Data: If show picture three-year-old ask tree , will likely get correct answer. If ask thirty year old definition tree , will likely get inconclusive answer. We learn tree studying [model] trees []. We learned looking trees. In words, learned data. In words, three year old forms model constitutes tree process discerning common pattern objects grown-ups around label "trees." ( data). She can "predict" something ( ) tree applying model new instances presented . This exactly happens machine learning: computer ( correctly, algorithm) builds predictive model variable (like "treeness") based patterns discerns data. The model can applied predict value variable (e.g. tree ) new instances. With said introduction, worth contrasting machine-driven process model building traditional approach building mathematical models predict phenomena , say, physics engineering. What models good ? Physicists engineers model phenomena using physical laws mathematics. The aim modelling understand predict natural phenomena. For example, physical law Newton's Law Gravitation model   helps us understand gravity works make predictions (say) Mars going six months now. Indeed, theories laws physics models wide applicability. (Aside: Models typically expressed via differential equations. Most differential equations hard solve analytically ( exactly), scientists use computers solve numerically. It important note case computers used calculation tools, play role model-building.) As mentioned earlier, role models sciences twofold   understanding prediction. In contrast, machine learning focus usually prediction rather understanding. The predictive successes machine learning led certain commentators claim scientific theory building obsolete science can advance crunching data alone. Such claims overblown, mention, hubristic, although data scientist may able predict accuracy, may able tell particular prediction obtained. This lack understanding can mislead can even harmful consequences, point worth unpacking detail  Assumptions, assumptions A model real world process phenomenon necessarily simplification. This essentially impossible isolate process phenomenon rest world. As consequence impossible know certain model one built incorporated interactions influence process / phenomenon interest. It quite possible potentially important variables overlooked. The selection variables go model based assumptions. In case model building physics, assumptions made upfront thus clear anybody takes trouble read underlying theory. In machine learning, however, assumptions harder see implicit data algorithm. This can problem data biased algorithm opaque. Problem bias opacity become acute datasets increase size algorithms become complex, especially applied social issues serious human consequences. I go , examples interested reader may want look Cathy O'Neil's book, Weapons Math Destruction, article dark side data science. As aside, I point although assumptions usually obvious traditional modelling, often overlooked sheer laziness , charitably, lack awareness. This can disastrous consequences. The global financial crisis 2008 can   extent   blamed failure trading professionals understand assumptions behind model used calculate value collateralised debt obligations. It starts straight line . Now taken tour key differences model building old new worlds, set start talking machine learning proper. I begin admitting I overstated point opacity: machine learning algorithms transparent can possibly . Indeed, chances know algorithm I'm going discuss next, either introductory statistics course university plotting relationships two variables favourite spreadsheet. Yea, may guessed I'm referring linear regression. In simplest avatar, linear regression attempts fit straight line set data points two dimensions. The two dimensions correspond dependent variable (traditionally denoted y) independent variable (traditionally denoted x). An example fitted line shown Figure 1. Once line obtained, one can "predict" value dependent variable value independent variable. In terms earlier discussion, line model. Figure 1: Linear Regression Figure 1: Linear Regression Figure 1 also serves illustrate linear models going inappropriate real world situations ( straight line fit data well). But hard devise methods fit complicated functions. The important point since machine learning finding functions accurately predict dependent variables yet unknown values independent variables, algorithms make explicit implicit choices form functions. Complexity versus simplicity At first sight seems -brainer complicated functions will work better simple ones. After , choose nonlinear function lots parameters, able fit complex data set better linear function can (See Figure 2   complicated function fits datapoints better straight line). But catch: although ability fit dataset increases flexibility fitting function, increasing complexity beyond point will invariably reduce predictive power. Put another way, complex enough function may fit known data points perfectly , consequence, will inevitably perform poorly unknown data. This important point look greater detail. Figure 2: Simple complex fitting functions Figure 2: Simple complex fitting function (courtesy: Wikimedia) Recall aim machine learning predict values dependent variable yet unknown values independent variable(s). Given finite ( usually, limited) dataset, build model can confidence ? The usual strategy partition dataset two subsets, one containing 60 80% data (called training set) containing remainder (called test set). The model built   .e. appropriate function fitted   using training data verified test data. The verification process consists comparing predicted values dependent variable known values test set. Now, intuitively clear complicated function, better will fit training data. Question: Why? Answer: Because complicated functions free parameters   example, linear functions single (dependent) variable two parameters (slope intercept), quadratics three, cubics four . The mathematician, John von Neumann believed said, "With four parameters I can fit elephant, five I can make wiggle trunk." See post nice demonstration literal truth words. Put another way, complex functions wrigglier simple ones,   suitable adjustment parameters   "wriggliness" can adjusted fit training data better functions less wriggly. Figure 2 illustrates point well. This may sound like can cake eat : choose complicated enough function can fit training test data well. Not ! Keep mind resulting model (fitted function) built using training set alone, good fit test data guaranteed. In fact, intuitively clear function fits training data perfectly ( Figure 2) likely terrible job test data. Question: Why? Answer: Remember, far model concerned, test data unknown. Hence, greater wriggliness trained model, less likely fit test data well. Remember, model fitted training data, freedom tweak parameters . This tension simplicity complexity models one key principles machine learning called bias-variance tradeoff. Bias refers lack flexibility variance, reducible error. In general simpler functions greater bias lower variance complex functions, opposite. Much subtlety machine learning lies developing understanding arrive right level complexity problem hand   , tweak parameters resulting function fits training data just well enough generalise well unknown data. Note: curious learn bias-variance tradeoff may want look piece. For details achieve optimal tradeoff, search articles regularization machine learning. Unlocking unstructured data The discussion thus far focused primarily quantitative enumerable data (numbers categories) stored structured format   .e. columns rows spreadsheet database table). This fine goes, fact much data organisations unstructured, common examples text documents audio-visual media. This data virtually impossible analyse computationally using relational database technologies ( SQL) commonly used organisations. The situation changed dramatically last decade . Text analysis techniques required expensive software high-end computers now implemented open source languages Python R, can run personal computers. For problems require computing power memory beyond , cloud technologies make possible cheaply. In opinion, ability analyse textual data important advance data technologies last decade . It unlocks world possibilities curious data analyst. Just think, comment fields survey data can now analysed way never possible relational world! There general impression text analysis hard. Although advanced techniques can take little time wrap one's head around, basics simple enough. Yea, I really mean   proof, check tutorial topic. Wrapping I go . Indeed, I planning delve algorithms increasing complexity ( regression trees forests neural nets) close brief peek recent headline-grabbing developments like deep learning. However, I realised exploration long (perhaps importantly) defeat main intent piece give starting students idea machine learning , differs preexisting techniques data analysis. I hope I succeeded, least partially, achieving aim. For interested learning machine learning algorithms, I can suggest look "Gentle Introduction Data Science using R" series articles. Start one text analysis (link last line previous section) move clustering, topic modelling, naive Bayes, decision trees, random forests support vector machines. I'm slowly adding list I find time, please check back time time. 
