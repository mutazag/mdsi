The standard way search documents internet via keywords keyphrases. This pretty much Google search engines routinely  well. However, useful , limitations. Consider, example, situation confronted large collection documents idea . One first things might want classify documents topics themes. Among things help figure anything interest also directing relevant subset(s) corpus. For small collections, one simply going document clearly infeasible corpuses containing thousands documents. Topic modeling   theme post   deals problem automatically classifying sets documents themes The article organised follows: I first provide background topic modelling. The algorithm I use, Latent Dirichlet Allocation (LDA), involves pretty heavy maths I'll avoid altogether. However, I will provide intuitive explanation LDA works moving practical example uses topicmodels library R. As previous articles series (see post one), I will discuss steps detail along explanations provide accessible references concepts covered space blog post. (Aside: Beware, LDA also abbreviation Linear Discriminant Analysis classification technique I hope cover later ongoing series text data analytics). Latent Dirichlet Allocation   math-free introduction In essence, LDA technique facilitates automatic discovery themes collection documents. The basic assumption behind LDA documents collection consist mixture collection-wide topics. However, reality observe documents words, topics   latter part hidden ( latent) structure documents. The aim infer latent topic structure given words document. LDA recreating documents corpus adjusting relative importance topics documents words topics iteratively. Here's brief explanation algorithm works, quoted directly answer Edwin Chen Quora: Go document, randomly assign word document one K topics. (Note: One shortcomings LDA one specify number topics, denoted K, upfront. More later.) This assignment already gives topic representations documents word distributions topics (albeit good ones). So improve , document d   .Go word w d    ..And topic t, compute two things: 1) p(topic t   document d) = proportion words document d currently assigned topic t, 2) p(word w   topic t) = proportion assignments topic t documents come word w. Reassign w new topic, choose topic t probability p(topic t   document d) * p(word w   topic t) (according generative model, essentially probability topic t generated word w, makes sense resample current word's topic probability). (Note: p( b) conditional probability given b already occurred   see post conditional probabilities)   ..In words, step, assuming topic assignments except current word question correct, updating assignment current word using model documents generated. After repeating previous step large number times, eventually reach roughly steady state assignments pretty good. So use assignments estimate topic mixtures document ( counting proportion words assigned topic within document) words associated topic ( counting proportion words assigned topic overall). For another simple explanation LDA works , check article Matthew Jockers. For technical exposition, take look video David Blei, one inventors algorithm. The iterative process described last point implemented using technique called Gibbs sampling. I'll say bit Gibbs sampling later, may want look paper Philip Resnick Eric Hardesty explains nitty-gritty algorithm (Warning: involves fair bit math, good intuitive explanations well). As general point, I also emphasise need understand ins outs algorithm use help understand, least high level, algorithm . One needs develop feel algorithms even one understand details. Indeed, people working analytics know details algorithms use, stop using algorithms intelligently. Purists may disagree. I think wrong. Finally   doubt wondering   term "Dirichlet" LDA refers fact topics words assumed follow Dirichlet distributions. There "good" reason apart convenience   Dirichlet distributions provide good approximations word distributions documents , perhaps important, computationally convenient. Preprocessing As previous articles text mining, I will use collection 30 posts blog example corpus. The corpus can downloaded . I will assume R RStudio installed. Follow link need help . The preprocessing steps much described previous articles. Nevertheless, I'll risk boring detailed listing can reproduce results : #load text mining library library(tm) #set working directory (modify path needed) setwd("C:\\Users\\Kailash\\Documents\\TextMining") #load files corpus #get listing .txt files directory filenames <- list.files(getwd(),pattern="*.txt") #read files character vector files <- lapply(filenames,readLines) #create corpus vector docs <- Corpus(VectorSource(files)) #inspect particular document corpus writeLines(.character(docs[[30]])) #start preprocessing #Transform lower case docs <-tm_map(docs,content_transformer(tolower)) #remove potentially problematic symbols toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, "'") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, "•") docs <- tm_map(docs, toSpace, """) docs <- tm_map(docs, toSpace, """) #remove punctuation docs <- tm_map(docs, removePunctuation) #Strip digits docs <- tm_map(docs, removeNumbers) #remove stopwords docs <- tm_map(docs, removeWords, stopwords("english")) #remove whitespace docs <- tm_map(docs, stripWhitespace) #Good practice check every now writeLines(.character(docs[[30]])) #Stem document docs <- tm_map(docs,stemDocument) #fix 1) differences us aussie english 2) general errors docs <- tm_map(docs, content_transformer(gsub), pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") #define eliminate custom stopwords myStopwords <- c("can", "say","one","way","use", "also","howev","tell","will", "much","need","take","tend","even", "like","particular","rather","said", "get","well","make","ask","come","end", "first","two","help","often","may", "might","see","someth","thing","point", "post","look","right","now","think","‘ve ", "‘re ","anoth","put","set","new","good", "want","sure","kind","larg","yes,","day","etc", "quit","sinc","attempt","lack","seen","awar", "littl","ever","moreov","though","found","abl", "enough","far","earli","away","achiev","draw", "last","never","brief","bit","entir","brief", "great","lot") docs <- tm_map(docs, removeWords, myStopwords) #inspect document check writeLines(.character(docs[[30]])) #Create document-term matrix dtm <- DocumentTermMatrix(docs) #convert rownames filenames rownames(dtm) <- filenames #collapse matrix summing columns freq <- colSums(.matrix(dtm)) #length total number terms length(freq) #create sort order (descending) ord <- order(freq,decreasing=TRUE) #List terms decreasing order freq write disk freq[ord] write.csv(freq[ord],"word_freq.csv") Check preprocessing section either article one detailed explanations code. The document term matrix (DTM) produced code will main input LDA algorithm next section. Topic modelling using LDA We now ready topic modelling. We'll use topicmodels package written Bettina Gruen Kurt Hornik. Specifically, use LDA function Gibbs sampling option mentioned earlier, I'll say second. The LDA function fairly large number parameters. I'll describe briefly . For , please check vignette Gruen Hornik. For part, use default parameter values supplied LDA function,custom setting parameters required Gibbs sampling algorithm. Gibbs sampling works performing random walk way reflects characteristics desired distribution. Because starting point walk chosen random, necessary discard first steps walk ( correctly reflect properties distribution). This referred burn- period. We set burn- parameter 4000. Following burn- period, perform 2000 iterations, taking every 500th iteration use. The reason avoid correlations samples. We use 5 different starting points (nstart=5)   , five independent runs. Each starting point requires seed integer ( also ensures reproducibility), I provided 5 random integers seed list. Finally I've set best TRUE (actually default setting), instructs algorithm return results run highest posterior probability. Some words caution order . It emphasised settings guarantee convergence algorithm globally optimal solution. Indeed, Gibbs sampling will, best, find locally optimal solution, even hard prove mathematically specific practical problems one dealing . The upshot best lots runs different settings parameters check stability results. The bottom line interest purely practical good enough results make sense. We'll leave issues mathematical rigour better qualified deal As mentioned earlier, important parameter must specified upfront: k, number topics algorithm use classify documents. There mathematical approaches , often yield semantically meaningful choices k (see post stackoverflow example). From practical point view, one can simply run algorithm different values k make choice based inspecting results. This . OK, first step set parameters R  , also load topicmodels library (Note: might need install package part base R installation). #load topic models library library(topicmodels) #Set parameters Gibbs sampling burnin <- 4000 iter <- 2000 thin <- 500 seed <-list(2003,5,63,100001,765) nstart <- 5 best <- TRUE #Number topics k <- 5 That done, can now actual work   run topic modelling algorithm corpus. Here code: #Run LDA using Gibbs sampling ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin)) #write results #docs topics ldaOut.topics <- .matrix(topics(ldaOut)) write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv")) #top 6 terms topic ldaOut.terms <- .matrix(terms(ldaOut,6)) write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv")) #probabilities associated topic assignment topicProbabilities <- .data.frame(ldaOut@gamma) write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv")) #Find relative importance top 2 topics topic1ToTopic2 <- lapply(1:nrow(dtm),function(x) sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1]) #Find relative importance second third important topics topic2ToTopic3 <- lapply(1:nrow(dtm),function(x) sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2]) #write file write.csv(topic1ToTopic2,file=paste("LDAGibbs",k,"Topic1ToTopic2.csv")) write.csv(topic2ToTopic3,file=paste("LDAGibbs",k,"Topic2ToTopic3.csv")) The LDA algorithm returns object contains lot information. Of particular interest us document topic assignments, top terms topic probabilities associated terms. These printed first three calls write.csv . There important points note : Each document considered mixture topics (5 case). The assignments first file list top topic   , one highest probability ( point 3 ). Each topic contains terms (words) corpus, albeit different probabilities. We list top 6 terms second file. The last file lists probabilities topic assigned document. This therefore 30 x 5 matrix   30 docs 5 topics. As one might expect, highest probability row corresponds topic assigned document. The "goodness" primary assignment ( discussed point 1) can assessed taking ratio highest second-highest probability second-highest third-highest probability . This I've done last nine lines code . Take time examine output confirm primary topic assignments best ratios probabilities discussed point 3 highest. You also experiment different values k see can find better topic distributions. In interests space I will restrict k = 5. The table lists top 6 terms topics 1 5. Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 1 work question chang system project 2 practic map organ data manag 3 mani time consult model approach 4 flexibl ibi manag design organ 5 differ issu work process decis 6 best plan problem busi problem The table lists document (primary) topic assignments: Document Topic BeyondEntitiesAndRelationships.txt 4 bigdata.txt 4 ConditionsOverCauses.txt 5 EmergentDesignInEnterpriseIT.txt 4 FromInformationToKnowledge.txt 2 FromTheCoalface.txt 1 HeraclitusAndParmenides.txt 3 IroniesOfEnterpriseIT.txt 3 MakingSenseOfOrganizationalChange.txt 5 MakingSenseOfSensemaking.txt 2 ObjectivityAndTheEthicalDimensionOfDecisionMaking.txt 5 OnTheInherentAmbiguitiesOfManagingProjects.txt 5 OrganisationalSurprise.txt 5 ProfessionalsOrPoliticians.txt 3 RitualsInInformationSystemDesign.txt 4 RoutinesAndReality.txt 4 ScapegoatsAndSystems.txt 5 SherlockHolmesFailedProjects.txt 3 sherlockHolmesMgmtFetis.txt 3 SixHeresiesForBI.txt 4 SixHeresiesForEnterpriseArchitecture.txt 3 TheArchitectAndTheApparition.txt 3 TheCloudAndTheGrass.txt 2 TheConsultantsDilemma.txt 3 TheDangerWithin.txt 5 TheDilemmasOfEnterpriseIT.txt 3 TheEssenceOfEntrepreneurship.txt 1 ThreeTypesOfUncertainty.txt 5 TOGAFOrNotTOGAF.txt 3 UnderstandingFlexibility.txt 1 From quick perusal two tables appears algorithm done pretty decent job. For example,topic 4 data system design, documents assigned topic. However, far perfect   example, interview I Neil Preston organisational change (MakingSenseOfOrganizationalChange.txt) assigned topic 5, seems project management. It associated Topic 3, change. Let's see can resolve looking probabilities associated topics. The table lists topic probabilities document: Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 BeyondEn 0.071 0.064 0.024 0.741 0.1 bigdata. 0.182 0.221 0.182 0.26 0.156 Conditio 0.144 0.109 0.048 0.205 0.494 Emergent 0.121 0.226 0.204 0.236 0.213 FromInfo 0.096 0.643 0.026 0.169 0.066 FromTheC 0.636 0.082 0.058 0.086 0.138 Heraclit 0.137 0.091 0.503 0.162 0.107 IroniesO 0.101 0.088 0.388 0.26 0.162 MakingSe 0.13 0.206 0.262 0.089 0.313 MakingSe 0.09 0.715 0.055 0.067 0.074 Objectiv 0.216 0.078 0.086 0.242 0.378 OnTheInh 0.18 0.234 0.102 0.12 0.364 Organisa 0.089 0.095 0.07 0.092 0.655 Professi 0.155 0.064 0.509 0.128 0.144 RitualsI 0.103 0.064 0.044 0.676 0.112 Routines 0.108 0.042 0.033 0.69 0.127 Scapegoa 0.135 0.088 0.043 0.185 0.549 Sherlock 0.093 0.082 0.398 0.195 0.232 sherlock 0.108 0.136 0.453 0.123 0.18 SixHeres 0.159 0.11 0.078 0.516 0.138 SixHeres 0.104 0.111 0.366 0.212 0.207 TheArchi 0.111 0.221 0.522 0.088 0.058 TheCloud 0.185 0.333 0.198 0.136 0.148 TheConsu 0.105 0.184 0.518 0.096 0.096 TheDange 0.114 0.079 0.037 0.079 0.69 TheDilem 0.125 0.128 0.389 0.261 0.098 TheEssen 0.713 0.059 0.031 0.113 0.084 ThreeTyp 0.09 0.076 0.042 0.083 0.708 TOGAFOrN 0.158 0.232 0.352 0.151 0.107 Understa 0.658 0.065 0.072 0.101 0.105 In table, highest probability row bold. Also, cases maximum second/third largest probabilities close, I highlighted second ( third) highest probabilities red. It clear Neil's interview (9th document table) 3 topics comparable probabilities   topic 5 (project management), topic 3 (change) topic 2 (issue mapping / ibis), decreasing order probabilities. In general, document multiple topics comparable probabilities, simply means document speaks topics proportions indicated probabilities. A reading Neil's interview will convince conversation indeed range topics. That said, algorithm far perfect. You might already noticed poor assignments. Here one   post Sherlock Holmes case failed project assigned topic 3; I reckon belongs topic 5. There number others, I belabor point, except reiterate precisely definitely want experiment different settings iteration parameters ( check stability) , important, try range different values k find optimal number topics. To conclude Topic modelling provides quick convenient way perform unsupervised classification corpus documents. As always, though, one needs examine results carefully check make sense. I'd like end general observation. Classifying documents age-old concern cuts across disciplines. So surprise topic modelling got look- diverse communities. Indeed, I reading learning LDA, I found best introductory articles area written academics working English departments! This one things I love working text analysis, wealth material web written diverse perspectives. The term cross-disciplinary often tends platitude , case simply statement fact. I hope I able convince explore rapidly evolving field. Exciting times ahead, come join fun.
