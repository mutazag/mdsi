Welcome second part introductory series text analysis using R ( first article can accessed ). My aim present piece provide practical introduction cluster analysis. I'll begin background moving nuts bolts clustering. We fair bit cover, get right . A common problem analysing large collections documents categorize meaningful way. This easy enough one predefined classification scheme known fit collection ( collection small enough browsed manually). One can simply scan documents, looking keywords appropriate category classify documents based results. More often , however, classification scheme available collection large. One needs use algorithms can classify documents automatically based structure content. The present post practical introduction couple automatic text categorization techniques, often referred clustering algorithms. As Wikipedia article clustering tells us: Cluster analysis clustering task grouping set objects way objects group (called cluster) similar ( sense another) groups (clusters). As one might guess , results clustering depend rather critically method one uses group objects. Again, quoting Wikipedia piece: Cluster analysis one specific algorithm, general task solved. It can achieved various algorithms differ significantly notion constitutes cluster efficiently find . Popular notions clusters include groups small distances [Note: use distance-based methods] among cluster members, dense areas data space, intervals particular statistical distributions [.e. distributions words within documents entire collection].   bit later:   notion "cluster" precisely defined, one reasons many clustering algorithms. There common denominator: group data objects. However, different researchers employ different cluster models, cluster models different algorithms can given. The notion cluster, found different algorithms, varies significantly properties. Understanding "cluster models" key understanding differences various algorithms. An upshot always straightforward interpret output clustering algorithms. Indeed, will see example discussed . With said introduction, move nut bolts clustering. Preprocessing corpus In section I cover steps required create R objects necessary order clustering. It goes territory I've covered detail first article series   albeit tweaks, may want skim even read previous piece. To begin I'll assume R RStudio ( free development environment R) installed computer familiar basic functionality text mining ™ package. If need help , please look instructions previous article text mining. As first part series, I will use 30 posts blog example collection ( corpus, text mining-speak). The corpus can downloaded . For completeness, I will run entire sequence steps   right loading corpus R, running two clustering algorithms. Ready? Let's go  The first step fire RStudio navigate directory unpacked example corpus. Once done, load text mining package, tm. Here's relevant code (Note: complete listing code article can accessed ): getwd() [1] "C:/Users/Kailash/Documents" #set working directory   fix path needed! setwd("C:/Users/Kailash/Documents/TextMining") #load tm library library(tm) Loading required package: NLP Note: R commands blue, output black red; lines start # comments. If get error , probably need download install tm package. You can RStudio going Tools > Install Packages entering "tm". When installing new package, R automatically checks installs dependent packages. The next step load collection documents object can manipulated functions tm package. #Create Corpus docs <- Corpus(DirSource("C:/Users/Kailash/Documents/TextMining")) #inspect particular document writeLines(.character(docs[[30]]))   The next step clean corpus. This includes things transforming consistent case, removing non-standard symbols & punctuation, removing numbers (assuming numbers contain useful information, case ): #Transform lower case docs <- tm_map(docs,content_transformer(tolower)) #remove potentiallyy problematic symbols toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, ":") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, "•") docs <- tm_map(docs, toSpace, "• ") docs <- tm_map(docs, toSpace, " -") docs <- tm_map(docs, toSpace, """) docs <- tm_map(docs, toSpace, """) #remove punctuation docs <- tm_map(docs, removePunctuation) #Strip digits docs <- tm_map(docs, removeNumbers) Note: please see previous article content_transformer toSpace function defined . Next remove stopwords   common words (like "" "" "", example) eliminate extraneous whitespaces. #remove stopwords docs <- tm_map(docs, removeWords, stopwords("english")) #remove whitespace docs <- tm_map(docs, stripWhitespace) writeLines(.character(docs[[30]])) flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun At point critical inspect corpus stopword removal tm can flaky. Yes, annoying showstopper one can remove problematic words manually one identified   minute. Next, stem document   .e. truncate words base form. For example, "education", "educate" "educative" stemmed "educat.": docs <- tm_map(docs,stemDocument) Stemming works well enough, fixes need done due inconsistent use British/Aussie US English. Also, take opportunity fix concatenations like "andgovernance" (see paragraph printed ). Here's code: docs <- tm_map(docs, content_transformer(gsub),pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") The next step remove stopwords missed R. The best way small corpus go compile list words eliminated. One can create custom vector containing words removed use removeWords transformation needful. Here code (Note: + indicates continuation statement previous line): myStopwords <- c("can", "say","one","way","use", + "also","howev","tell","will", + "much","need","take","tend","even", + "like","particular","rather","said", + "get","well","make","ask","come","end", + "first","two","help","often","may", + "might","see","someth","thing","point", + "post","look","right","now","think","'ve ", + "'re ") #remove custom stopwords docs <- tm_map(docs, removeWords, myStopwords) Again, good idea check offending words really eliminated. The final preprocessing step create document-term matrix (DTM)   matrix lists occurrences words corpus. In DTM, documents represented rows terms ( words) columns. If word occurs particular document n times, matrix entry corresponding row column n, occur , entry 0. Creating DTM straightforward  one simply uses built- DocumentTermMatrix function provided tm package like : dtm <- DocumentTermMatrix(docs) #print summary dtm Non-/sparse entries: 13312/110618 Sparsity : 89% Maximal term length: 48 Weighting : term frequency (tf) This brings us end preprocessing phase. Next, I'll briefly explain distance-based algorithms work going actual work clustering. An intuitive introduction algorithms As mentioned introduction, basic idea behind document text clustering categorise documents groups based likeness. Let's take brief look algorithms work magic. Consider structure DTM. Very briefly, matrix documents represented rows words columns. In case, corpus 30 documents 4131 words, DTM 30 x 4131 matrix. Mathematically, one can think matrix describing 4131 dimensional space words represents coordinate axis document represented point space. This hard visualise course, may help illustrate via two-document corpus three words total. Consider following corpus: Document A: "five plus five" Document B: "five plus six" These two documents can represented points 3 dimensional space words "five" "plus" "six" three coordinate axes (see figure 1). Figure 1: Documents A B points 3-word space Figure 1: Documents A B points 3-word space Now, documents can thought point space, easy enough take next logical step define notion distance two points (.e. two documents). In figure 1 distance A B ( I denote D(A,B)) length line connecting two points, simply, sum squares differences coordinates two points representing documents. D(A,B) = \sqrt{(2-1)^2 + (1-1)^2+(0-1)^2} = \sqrt 2 Generalising 4131 dimensional space hand, distance two documents ( call X Y) coordinates (word frequencies) (x_1,x_2,...x_{4131}) (y_1,y_2,...y_{4131}), one can define straight line distance (also called Euclidean distance) D(X,Y) : D(X,Y) = \sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2+...+(x_{4131} - y_{4131})^2} It noted Euclidean distance I described possible way define distance mathematically. There many others take far afield discuss   see article ( put term metric, metric context merely distance) What's important idea one can define numerical distance documents. Once grasped, easy understand basic idea behind () clustering algorithms work   group documents based distance-related criteria. To sure, explanation simplistic glosses complicated details algorithms. Nevertheless reasonable, approximate explanation goes hood. I hope purists reading will agree! Finally, completeness I mention many clustering algorithms , distance-based. Hierarchical clustering The first algorithm look hierarchical clustering. As Wikipedia article topic tells us, strategies hierarchical clustering fall two types: Agglomerative: start document cluster. The algorithm iteratively merges documents clusters closest entire corpus forms single cluster. Each merge happens different (increasing) distance. Divisive: start entire set documents single cluster. At step algorithm splits cluster recursively document cluster. This basically inverse agglomerative strategy. The algorithm use hclust agglomerative hierarchical clustering. Here's simplified description works: Assign document (single member) cluster Find pair clusters closest merge . So now one cluster less . Compute distances new cluster old clusters. Repeat steps 2 3 single cluster containing documents. We'll need things running algorithm. Firstly, need convert DTM standard matrix can used dist, distance computation function R ( DTM stored standard matrix). We'll also shorten document names display nicely graph will use display results hclust ( names I given documents just way long). Here's relevant code: #convert dtm matrix m <- .matrix(dtm) #write csv file (optional) write.csv(m,file="dtmEight2Late.csv") #shorten rownames display purposes rownames(m) <- paste(substring(rownames(m),1,3),rep("..",nrow(m)), + substring(rownames(m), nchar(rownames(m))-12,nchar(rownames(m))-4)) #compute distance document vectors d <- dist(m) Next run hclust. The algorithm offers several options check documentation details. I use popular option called Ward's method   others, I suggest experiment gives slightly different results making interpretation somewhat tricky ( I mention clustering much art science??). Finally, visualise results dendogram (see Figure 2 ). #run hierarchical clustering using Ward's method groups <- hclust(d,method="ward.D") #plot dendogram, use hang ensure labels fall tree plot(groups, hang=-1) Figure 2: Dendogram hierarchical clustering corpus Figure 2: Dendogram hierarchical clustering corpus A words interpreting dendrograms hierarchical clusters: work way tree figure 2, branch point encounter distance cluster merge occurred. Clearly, well-defined clusters largest separation; many closely spaced branch points indicate lack dissimilarity (.e. distance, case) clusters. Based , figure reveals 2 well-defined clusters   first one consisting three documents right end cluster second containing documents. We can display clusters graph using rect.hclust function like : #cut 2 subtrees   try 3 5 rect.hclust(groups,2) The result shown figure . Figure 3: 2 cluster solution Figure 3: 2 cluster grouping The figures 4 5 show grouping 3, 5 clusters. Figure 4: 3 cluster solution Figure 4: 3 cluster grouping Figure 5: 5 cluster solution Figure 5: 5 cluster grouping I'll make just one point : 2 cluster grouping seems robust one happens large distance, cleanly separated (distance-wise) 3 5 cluster grouping. That said, I'll leave explore ins outs hclust move next algorithm. K means clustering In hierarchical clustering specify number clusters upfront. These determined looking dendogram algorithm done work. In contrast, next algorithm   K means   requires us define number clusters upfront ( number "k" name). The algorithm generates k document clusters way ensures within-cluster distances cluster member centroid ( geometric mean) cluster minimised. Here's simplified description algorithm: Assign documents randomly k bins Compute location centroid bin. Compute distance document centroid Assign document bin corresponding centroid closest . Stop document moved new bin, else go step 2. An important limitation k means method solution found algorithm corresponds local rather global minimum ( figure Wikipedia explains difference two nice succinct way). As consequence important run algorithm number times ( time different starting configuration) select result gives overall lowest sum within-cluster distances documents. A simple check solution robust run algorithm increasing number initial configurations result change significantly. That said, procedure guarantee globally optimal solution. I reckon enough said algorithm, get using . The relevant function, might well guessed kmeans. As always, I urge check documentation understand available options. We'll use default options parameters excepting nstart set 100. We also plot result using clusplot function cluster library ( may need install. Reminder can install packages via Tools>Install Packages menu RStudio) #k means algorithm, 2 clusters, 100 starting configurations kfit <- kmeans(d, 2, nstart=100) #plot   need library cluster library(cluster) clusplot(.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0) The plot shown Figure 6. Figure 6: principal component plot (k=2) Figure 6: principal component plot (k=2) The cluster plot shown figure needs bit explanation. As mentioned earlier, clustering algorithms work mathematical space whose dimensionality equals number words corpus (4131 case). Clearly, impossible visualize. To handle , mathematicians invented dimensionality reduction technique called Principal Component Analysis reduces number dimensions 2 ( case) way reduced dimensions capture much variability clusters possible ( hence comment, " two components explain 69.42% point variability" bottom plot figure 6) (Aside Yes I realize figures hard read overly long names, I leave fix . No excuses, know  :-)) Running algorithm plotting results k=3 5 yields figures . Figure 7: Principal component plot (k=3) Figure 7: Principal component plot (k=3) Figure 8: Principal component plot (k=5) Figure 8: Principal component plot (k=5) Choosing k Recall k means algorithm requires us specify k upfront. A natural question : best choice k? In truth one-size-fits- answer question, heuristics might sometimes help guide choice. For completeness I'll describe one even though much help clustering problem. In simplified description k means algorithm I mentioned technique attempts minimise sum distances points cluster cluster's centroid. Actually, quantity minimised total within-cluster sum squares (WSS) point mean. Intuitively one might expect quantity maximum k=1 decrease k increases, sharply first less sharply k reaches optimal value. The problem reasoning often happens within cluster sum squares never shows slowing decrease summed WSS. Unfortunately exactly happens case hand. I reckon picture might help make clearer. Below R code draw plot summed WSS function k k=2 way 29 (1-total number documents): #kmeans   determine optimum number clusters (elbow method) #look "elbow" plot summed intra-cluster distances (withinss) fn k wss <- 2:29 ( 2:29) wss[] <- sum(kmeans(d,centers=,nstart=25)$withinss) plot(2:29, wss[2:29], type="b", xlab="Number Clusters",ylab="Within groups sum squares")   figure shows resulting plot. Figure 10: WSS function k ("elbow plot") The plot clearly shows k summed WSS flattens ( distinct "elbow"). As result method help. Fortunately, case one can get sensible answer using common sense rather computation: choice 2 clusters seems optimal algorithms yield exactly clusters show clearest cluster separation point (review dendogram cluster plots k=2). The meaning Now I must acknowledge elephant room I steadfastly ignored thus far. The odds good seen already . It : topics themes (two) clusters correspond ? Unfortunately question straightforward answer. Although algorithms suggest 2-cluster grouping, silent topics themes related . Moreover, will see experiment, results clustering depend : The criteria construction DTM (see documentation DocumentTermMatrix options). The clustering algorithm . Indeed, insofar clustering concerned, subject matter corpus knowledge best way figure cluster themes. This serves reinforce (yet !) clustering much art science. In case hand, article length seems important differentiator 2 clusters found algorithms. The three articles smaller cluster top 4 longest pieces corpus. Additionally, three pieces related sensemaking dialogue mapping. There probably factors well, none stand significant. I mention, however, fact article length seems play significant role suggests may worth checking effect scaling distances word counts using measures cosine similarity   topic another post! (Note added Dec 3 2015: check article visualizing relationships documents using network graphs detailed discussion cosine similarity) The take home lesson results clustering often hard interpret. This surprising   algorithms interpret meaning, simply chug mathematical optimisation problem. The onus analyst figure means  means anything . Conclusion This brings us end long ramble clustering. We've explored two common methods: hierarchical k means clustering ( many others available R, I urge explore ). Apart providing detailed steps clustering, I attempted provide intuitive explanation algorithms work. I hope I succeeded . As always feedback welcome. Finally, I'd like reiterate important point: results clustering exercise straightforward interpretation, often case cluster analysis. Fortunately I can close optimistic note. There text mining techniques better job grouping documents based topics themes rather word frequencies alone. I'll discuss next article series. Until , I wish many enjoyable hours exploring ins outs clustering.
