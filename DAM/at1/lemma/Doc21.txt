standard way search document internet via keyword keyphrases pretty much google search engine routinely good however useful limitation consider example situation confront large collection document idea first thing may want classify document topic theme among thing help figure theres anything interest also direct relevant subset corpus for small collection simply go document clearly infeasible corpus contain thousand document topic model theme post deal problem automatically classify set document theme article organise follow i first provide background topic model algorithm i use latent dirichlet allocation lda involve pretty heavy math ill avoid altogether however i provide intuitive explanation lda work move practical example use topicmodels library r a previous article series see post i discuss step detail along explanation provide accessible reference concept cover space blog post aside beware lda also abbreviation linear discriminant analysis classification technique i hope cover late ongoing series text datum analytics latent dirichlet allocation mathfree introduction in essence lda technique facilitate automatic discovery theme collection document basic assumption behind lda document collection consist mixture collectionwide topic however reality observe document word topic latter part hide latent structure document aim infer latent topic structure give word document lda recreate document corpus adjust relative importance topic document word topic iteratively heres brief explanation algorithm work quote directly answer edwin chen quora go document randomly assign word document k topic note shortcoming lda specify numb topic denote k upfront much late assignment already give topic representation document word distribution topic albeit good one so improve document have go word w have and topic t compute two thing ptopic t document have proportion word document have currently assign topic t pword w topic t proportion assignment topic t document come word w reassign w new topic choose topic t probability ptopic t document have pword w topic t accord generative model essentially probability topic t generate word w make sense resample current word topic probability note pab conditional probability give b already occur see post conditional probability in word step assume topic assignment except current word question correct update assignment current word use model document generate after repeat previous step large numb time youll eventually reach roughly steady state assignment pretty good so use assignment estimate topic mixture document count proportion word assign topic within document word associate topic count proportion word assign topic overall for another simple explanation lda work check article matthew jockers for technical exposition take look video david blei inventor algorithm iterative process describe last point implement use technique call gibbs sample ill say bite gibbs sample late may want look paper philip resnick eric hardesty explain nittygritty algorithm warn involve fair bite math good intuitive explanation good a general point i also emphasise need understand ins out algorithm use help understand less high level algorithm need develop feel algorithm even doesnt understand detail indeed people work analytics know detail algorithm use doesnt stop use algorithm intelligently purist may disagree i think wrong finally youre doubt wonder term dirichlet lda refer fact topic word assume follow dirichlet distribution there good reason apart convenience dirichlet distribution provide good approximation word distribution document perhaps important computationally convenient preprocess a previous article text mine i use collection post blog example corpus corpus download i assume r rstudio install follow link need help preprocess step much describe previous article nevertheless ill risk bore detail list reproduce result load text mine library librarytm set work directory modify path need setwdcuserskailashdocumentstextmining load file corpus get list txt file directory filename listfilesgetwdpatterntxt read file character vector file lapplyfilenamesreadlines create corpus vector doc corpusvectorsourcefiles inspect particular document corpus writelinesascharacterdocs start preprocess transform low case doc tmmapdocscontenttransformertolower remove potentially problematic symbol tospace contenttransformerfunctionx pattern return gsubpattern x doc tmmapdocs tospace doc tmmapdocs tospace doc tmmapdocs tospace doc tmmapdocs tospace • doc tmmapdocs tospace doc tmmapdocs tospace remove punctuation doc tmmapdocs removepunctuation strip digit doc tmmapdocs removenumbers remove stopwords doc tmmapdocs removewords stopwordsenglish remove whitespace doc tmmapdocs stripwhitespace good practice check every now writelinesascharacterdocs stem document doc tmmapdocsstemdocument fix difference us aussie english general error doc tmmapdocs contenttransformergsub pattern organiz replacement organ doc tmmapdocs contenttransformergsub pattern organis replacement organ doc tmmapdocs contenttransformergsub pattern andgovern replacement govern doc tmmapdocs contenttransformergsub pattern inenterpris replacement enterpris doc tmmapdocs contenttransformergsub pattern team replacement team define eliminate custom stopwords mystopwords ccan sayonewayuse alsohowevtellwill muchneedtaketendeven likeparticularrathersaid getwellmakeaskcomeend firsttwohelpoftenmay mightseesomeththingpoint postlookrightnowthink have re anothputsetnewgood wantsurekindlargyesdayetc quitsincattemptlackseenawar littlevermoreovthoughfoundabl enoughfarearliawayachievdraw lastneverbriefbitentirbrief greatlot doc tmmapdocs removewords mystopwords inspect document check writelinesascharacterdocs create documentterm matrix dtm documenttermmatrixdocs convert rownames filename rownamesdtm filename collapse matrix sum column freq colsumsasmatrixdtm length total numb term lengthfreq create sort order descend ord orderfreqdecreasingtrue list term decrease order freq write disk freqord writecsvfreqordwordfreqcsv check preprocess section either article detail explanation code document term matrix dtm produce code main input lda algorithm next section topic model use lda we now ready topic model good use topicmodels package write bettina gruen kurt hornik specifically good use lda function gibbs sample option mention early ill say 2 lda function fairly large numb parameter ill describe briefly for please check vignette gruen hornik for part good use default parameter value supply lda functioncustom set parameter require gibbs sample algorithm gibbs sample work perform random walk way reflect characteristic desire distribution because start point walk choose random necessary discard first step walk correctly reflect property distribution refer burnin period we set burnin parameter follow burnin period perform iteration take every th iteration use reason avoid correlation sample we use different start point nstart five independent run each start point require seed integer also ensure reproducibility i provide random integer seed list finally ive set good true actually default set instruct algorithm return result run high posterior probability some word caution order it emphasise setting guarantee convergence algorithm globally optimal solution indeed gibbs sample good find locally optimal solution even hard prove mathematically specific practical problem deal upshot good lot run different setting parameter check stability result bottom line interest purely practical good enough result make sense good leave issue mathematical rigour good qualify deal a mention early important parameter must specify upfront k numb topic algorithm use classify document there mathematical approach often yield semantically meaningful choice k see post stackoverflow example from practical point view simply run algorithm different value k make choice base inspect result good okay first step set parameter r let also load topicmodels library note may need install package part base r installation load topic model library librarytopicmodels set parameter gibbs sample burnin iter thin seed list nstart good true numb topic k that do now actual work run topic model algorithm corpus here code run lda use gibbs sample ldaout ldadtmk methodgibbs controllistnstartnstart seed seed bestbest burnin burnin iter iter thinthin write result doc topic ldaouttopics asmatrixtopicsldaout writecsvldaouttopicsfilepasteldagibbskdocstotopicscsv top term topic ldaoutterms asmatrixtermsldaout writecsvldaouttermsfilepasteldagibbsktopicstotermscsv probability associate topic assignment topicprobabilities asdataframeldaoutgamma writecsvtopicprobabilitiesfilepasteldagibbsktopicprobabilitiescsv find relative importance top topic topictotopic lapplynrowdtmfunctionx sorttopicprobabilitiesxksorttopicprobabilitiesxk find relative importance 2 3 important topic topictotopic lapplynrowdtmfunctionx sorttopicprobabilitiesxksorttopicprobabilitiesxk write file writecsvtopictotopicfilepasteldagibbsktopictotopiccsv writecsvtopictotopicfilepasteldagibbsktopictotopiccsv lda algorithm return object contain lot information of particular interest us document topic assignment top term topic probability associate term this print first three call writecsv there important point note each document consider mixture topic case assignment first file list top topic high probability point each topic contain term word corpus albeit different probability we list top term 2 file last file list probability topic assign document therefore x matrix doc topic a may expect high probability row correspond topic assign document goodness primary assignment discuss point assess take ratio high secondhighest probability secondhighest thirdhighest probability ive do last nine line code take time examine output confirm primary topic assignment good ratio probability discuss point high you also experiment different value k see find good topic distribution in interest space i restrict k table list top term topic topic topic topic topic topic work question chang system project practice map organ datum manag mani time consult model approach flexibl ibi manag design organ differ issu work process decis good plan problem busi problem table list document primary topic assignment document topic beyondentitiesandrelationshipstxt bigdatatxt conditionsovercausestxt emergentdesigninenterpriseittxt frominformationtoknowledgetxt fromthecoalfacetxt heraclitusandparmenidestxt ironiesofenterpriseittxt makingsenseoforganizationalchangetxt makingsenseofsensemakingtxt objectivityandtheethicaldimensionofdecisionmakingtxt ontheinherentambiguitiesofmanagingprojectstxt organisationalsurprisetxt professionalsorpoliticianstxt ritualsininformationsystemdesigntxt routinesandrealitytxt scapegoatsandsystemstxt sherlockholmesfailedprojectstxt sherlockholmesmgmtfetistxt sixheresiesforbitxt sixheresiesforenterprisearchitecturetxt thearchitectandtheapparitiontxt thecloudandthegrasstxt theconsultantsdilemmatxt thedangerwithintxt thedilemmasofenterpriseittxt theessenceofentrepreneurshiptxt threetypesofuncertaintytxt togafornottogaftxt understandingflexibilitytxt from quick perusal two table appear algorithm do pretty decent job for exampletopic datum system design document assign topic however far perfect example interview i neil preston organisational change makingsenseoforganizationalchangetxt assign topic seem project management it associate topic change let see resolve look probability associate topic table list topic probability document topic topic topic topic topic beyonden bigdata conditio emergent frominfo fromthec heraclit ironieso makingse makingse objectiv ontheinh organisa professi ritualsi routine scapegoa sherlock sherlock sixheres sixheres thearchi thecloud theconsu thedange thedilem theessen threetyp togaforn understa in table high probability row bold also case maximum secondthird large probability close i highlight 2 3 high probability red it clear neils interview th document table topic comparable probability topic project management topic change topic issue map ibis decrease order probability in general document multiple topic comparable probability simply mean document speak topic proportion indicate probability a read neils interview convince conversation indeed range topic that say algorithm far perfect you may already notice poor assignment here post sherlock holmes case fail project assign topic i reckon belong topic there numb other i wont belabor point except reiterate precisely definitely want experiment different setting iteration parameter check stability important try range different value k find optimal numb topic to conclude topic model provide quick convenient way perform unsupervised classification corpus document a always though need examine result carefully check make sense id like end general observation classify document ageold concern cut across discipline so surprise topic model get lookin diverse community indeed i read learn lda i find good introductory article area write academic work english department thing i love work text analysis wealth material web write diverse perspective term crossdisciplinary often tend platitude case simply statement fact i hope i able convince explore rapidly evolve field excite time ahead come join fun
